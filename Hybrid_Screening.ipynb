{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Hybrid screening system: 3-day screening as an example**"
      ],
      "metadata": {
        "id": "8EWJZsZ0ZucA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Convert time difference to days\n",
        "long_3_df[\"time_difference_days\"] = (\n",
        "    pd.to_timedelta(long_3_df[\"time_difference\"]).dt.total_seconds() / 86400\n",
        ")\n",
        "\n",
        "\n",
        "# mark very high temporal risk among High symptoms\n",
        "rubric_df[\"is_veryhigh\"] = (\n",
        "    (rubric_df[\"symptom_risk_category\"] == \"High\") &\n",
        "    (rubric_df[\"stroke_risk_score\"] >= 0.85)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "2Ikofhg3ZucF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "long_3_df = long_3_df.merge(\n",
        "    rubric_df[[\"symptom_id\", \"symptom_risk_category\", \"is_veryhigh\"]],\n",
        "    left_on=\"classifications\",\n",
        "    right_on=\"symptom_id\",\n",
        "    how=\"left\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "F-KT5v7EZucG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compute_3day_screening_features_full(\n",
        "    df: pd.DataFrame,\n",
        "    person_col: str = \"user_id\",\n",
        "    days_col: str = \"time_difference_days\",\n",
        "    category_col: str = \"symptom_risk_category\",\n",
        "    veryhigh_col: str = \"is_veryhigh\",\n",
        "    outcome_col: str = \"stroke_event\",\n",
        "    window_days: int = 3,   # <-- 3-day window by default\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build 3-day features for ALL person in df:\n",
        "      - Counts of High / Moderate / Moderate-low / Low / VeryHigh in 0–3 days\n",
        "      - Person-level stroke_event from the full df\n",
        "      - 3-day screening alert using a simple conservative rule (editable)\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------------\n",
        "    # 0) Person-level outcome table\n",
        "    # -----------------------------\n",
        "    if outcome_col not in df.columns:\n",
        "        raise KeyError(f\"'{outcome_col}' not found in df.\")\n",
        "\n",
        "    person_outcome = (\n",
        "        df.groupby(person_col)[outcome_col]\n",
        "          .max()\n",
        "          .reset_index()\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1) Restrict to 0–3 day window (for counts only)\n",
        "    # -----------------------------\n",
        "    window = df[(df[days_col] >= 0) & (df[days_col] <= window_days)].copy()\n",
        "\n",
        "    if window.empty:\n",
        "        counts = person_outcome.copy()\n",
        "        for col in [\n",
        "            \"High_count_3d\", \"Moderate_count_3d\",\n",
        "            \"ModerateLow_count_3d\", \"Low_count_3d\",\n",
        "            \"VeryHigh_count_3d\", \"total_symptoms_3d\",\n",
        "            \"screening_alert_3d\"\n",
        "        ]:\n",
        "            counts[col] = 0\n",
        "        return counts\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2) Category counts: (person, category) → count\n",
        "    # -----------------------------\n",
        "    cat_counts = (\n",
        "        pd.crosstab(window[person_col], window[category_col])\n",
        "          .rename_axis(index=person_col)\n",
        "          .reset_index()\n",
        "    )\n",
        "\n",
        "    for cat in [\"High\", \"Moderate\", \"Moderate-low\", \"Low\"]:\n",
        "        if cat not in cat_counts.columns:\n",
        "            cat_counts[cat] = 0\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3) Very-High counts\n",
        "    # -----------------------------\n",
        "    if veryhigh_col in window.columns:\n",
        "        vh = (\n",
        "            window[window[veryhigh_col] == True]\n",
        "            .groupby(person_col)\n",
        "            .size()\n",
        "            .rename(\"VeryHigh_count_3d\")\n",
        "        )\n",
        "        cat_counts = cat_counts.merge(vh, on=person_col, how=\"left\")\n",
        "        cat_counts[\"VeryHigh_count_3d\"] = cat_counts[\"VeryHigh_count_3d\"].fillna(0).astype(int)\n",
        "    else:\n",
        "        cat_counts[\"VeryHigh_count_3d\"] = 0\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4) Rename category columns\n",
        "    # -----------------------------\n",
        "    counts = cat_counts.rename(columns={\n",
        "        \"High\": \"High_count_3d\",\n",
        "        \"Moderate\": \"Moderate_count_3d\",\n",
        "        \"Moderate-low\": \"ModerateLow_count_3d\",\n",
        "        \"Low\": \"Low_count_3d\",\n",
        "    })\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5) Merge with ALL person (including no 0–3d messages)\n",
        "    # -----------------------------\n",
        "    full = person_outcome.merge(counts, on=person_col, how=\"left\")\n",
        "\n",
        "    for col in [\n",
        "        \"High_count_3d\", \"Moderate_count_3d\",\n",
        "        \"ModerateLow_count_3d\", \"Low_count_3d\",\n",
        "        \"VeryHigh_count_3d\"\n",
        "    ]:\n",
        "        if col not in full.columns:\n",
        "            full[col] = 0\n",
        "        full[col] = full[col].fillna(0).astype(int)\n",
        "\n",
        "    full[\"total_symptoms_3d\"] = (\n",
        "        full[\"High_count_3d\"]\n",
        "        + full[\"Moderate_count_3d\"]\n",
        "        + full[\"ModerateLow_count_3d\"]\n",
        "        + full[\"Low_count_3d\"]\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # 6) Apply your 3-day rule (conservative; adjust if desired)\n",
        "    # -----------------------------\n",
        "    def rule_3d(row):\n",
        "        vh = row[\"VeryHigh_count_3d\"]\n",
        "        h  = row[\"High_count_3d\"]\n",
        "        m  = row[\"Moderate_count_3d\"]\n",
        "\n",
        "        # Very-High Alert\n",
        "        if vh >= 1:\n",
        "            return \"Very-High Alert (3d)\"\n",
        "\n",
        "        # High-Risk Alert (tight rule for short window)\n",
        "        if (h >= 1) or ((h + m) >= 2):\n",
        "            return \"High-Risk Alert (3d)\"\n",
        "\n",
        "        # Moderate-Risk Alert\n",
        "        if m >= 1:\n",
        "            return \"Moderate-Risk Alert (3d)\"\n",
        "\n",
        "        return \"No Alert (3d)\"\n",
        "\n",
        "    full[\"screening_alert_3d\"] = full.apply(rule_3d, axis=1)\n",
        "\n",
        "    return full\n"
      ],
      "metadata": {
        "id": "rwOZ1yIMZucG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_3_df = compute_3day_screening_features_full(long_3_df)\n",
        "\n",
        "eval_3_df[\"stroke_event\"].value_counts()\n"
      ],
      "metadata": {
        "id": "r8UeEXPaZucG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression rule"
      ],
      "metadata": {
        "id": "D0K_lSswKegD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def fit_logistic_3day(eval_3_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Fit a logistic regression model for 3-day stroke risk using\n",
        "    symptom-risk counts and return:\n",
        "      - fitted model\n",
        "      - coefficient table (coef + odds ratios + intercept)\n",
        "      - (X_test, y_test, y_prob) on the held-out set\n",
        "      - ROC AUC on the test set\n",
        "    \"\"\"\n",
        "\n",
        "    features_3 = [\n",
        "        \"VeryHigh_count_3d\",\n",
        "        \"High_count_3d\",\n",
        "        \"Moderate_count_3d\",\n",
        "        \"ModerateLow_count_3d\",\n",
        "        \"Low_count_3d\",\n",
        "    ]\n",
        "\n",
        "    X = eval_3_df[features_3].astype(float).values\n",
        "    y = eval_3_df[\"stroke_event\"].astype(int).values\n",
        "\n",
        "    # Check columns\n",
        "    missing = [f for f in features_3 + [\"stroke_event\"] if f not in eval_3_df.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"Missing required columns in eval_3_df: {missing}\")\n",
        "\n",
        "    X = eval_3_df[features_3].astype(float).values\n",
        "    y = eval_3_df[\"stroke_event\"].astype(int).values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.3,\n",
        "        stratify=y,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    clf = LogisticRegression(max_iter=2000)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_prob = clf.predict_proba(X_test)[:, 1]\n",
        "    auc_roc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    # Build coefficient table, including intercept\n",
        "    coef_table = pd.DataFrame({\n",
        "        \"feature\": [\"intercept\"] + features_3,\n",
        "        \"coef\": np.concatenate([[clf.intercept_[0]], clf.coef_[0]]),\n",
        "        \"odds_ratio\": np.exp(np.concatenate([[clf.intercept_[0]], clf.coef_[0]])),\n",
        "    })\n",
        "\n",
        "    return clf, coef_table, (X_test, y_test, y_prob), auc_roc\n"
      ],
      "metadata": {
        "id": "XNHOX07zZucG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf, coef_table, (X_test, y_test, y_prob), auc_roc = fit_logistic_3day(eval_3_df)\n",
        "\n",
        "print(coef_table)\n",
        "print(\"ROC AUC (3d):\", auc_roc)\n"
      ],
      "metadata": {
        "id": "WXFPqZsKZucG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression derived threshold optimization\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "def add_logistic_score_3d(eval_3_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = eval_3_df.copy()\n",
        "\n",
        "    for col in [\n",
        "        \"VeryHigh_count_3d\",\n",
        "        \"High_count_3d\",\n",
        "        \"Moderate_count_3d\",\n",
        "        \"ModerateLow_count_3d\",\n",
        "        \"Low_count_3d\",\n",
        "    ]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0\n",
        "\n",
        "    vh = df[\"VeryHigh_count_3d\"].astype(float)\n",
        "    h  = df[\"High_count_3d\"].astype(float)\n",
        "    m  = df[\"Moderate_count_3d\"].astype(float)\n",
        "    ml = df[\"ModerateLow_count_3d\"].astype(float)\n",
        "    l  = df[\"Low_count_3d\"].astype(float)\n",
        "\n",
        "    logit_3 = (\n",
        "        a\n",
        "        + b * vh\n",
        "        + c * h\n",
        "        + d * m\n",
        "        + e * ml\n",
        "        + f\n",
        "        * l\n",
        "    )\n",
        "    p_3 = 1.0 / (1.0 + np.exp(-logit_3))\n",
        "\n",
        "    df[\"logit_3\"] = logit_3\n",
        "    df[\"p_3\"] = p_3\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def threshold_scan(y_true, y_prob, thresholds=None, prevalence=0.10):\n",
        "    \"\"\"\n",
        "    Return a DataFrame of tn/fp/fn/tp, sensitivity, specificity, precision, npv, f1, accuracy\n",
        "    plus prevalence-adjusted PPV/NPV (Bayes) using a fixed prevalence.\n",
        "    \"\"\"\n",
        "    if thresholds is None:\n",
        "        thresholds = np.linspace(0, 1, 101)\n",
        "\n",
        "    rows = []\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    y_prob = np.asarray(y_prob).astype(float)\n",
        "\n",
        "    pi = float(prevalence)\n",
        "\n",
        "    for thr in thresholds:\n",
        "        y_hat = (y_prob >= thr).astype(int)\n",
        "\n",
        "        tp = ((y_true == 1) & (y_hat == 1)).sum()\n",
        "        tn = ((y_true == 0) & (y_hat == 0)).sum()\n",
        "        fp = ((y_true == 0) & (y_hat == 1)).sum()\n",
        "        fn = ((y_true == 1) & (y_hat == 0)).sum()\n",
        "\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
        "        precision   = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
        "        npv         = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
        "\n",
        "        # Bayes prevalence-adjusted PPV/NPV\n",
        "        if np.isnan(sensitivity) or np.isnan(specificity):\n",
        "            ppv_adj = np.nan\n",
        "            npv_adj = np.nan\n",
        "        else:\n",
        "            denom_ppv = sensitivity * pi + (1 - specificity) * (1 - pi)\n",
        "            ppv_adj = (sensitivity * pi) / denom_ppv if denom_ppv > 0 else np.nan\n",
        "\n",
        "            denom_npv = (1 - sensitivity) * pi + specificity * (1 - pi)\n",
        "            npv_adj = (specificity * (1 - pi)) / denom_npv if denom_npv > 0 else np.nan\n",
        "\n",
        "        f1 = (\n",
        "            2 * precision * sensitivity / (precision + sensitivity)\n",
        "            if not np.isnan(precision) and not np.isnan(sensitivity) and (precision + sensitivity) > 0\n",
        "            else np.nan\n",
        "        )\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp,\n",
        "            \"sensitivity\": sensitivity,\n",
        "            \"specificity\": specificity,\n",
        "            \"precision\": precision,\n",
        "            \"npv\": npv,\n",
        "            \"ppv_adj_prev_0.10\": ppv_adj,\n",
        "            \"npv_adj_prev_0.10\": npv_adj,\n",
        "            \"f1\": f1,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"threshold\": thr,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "\n",
        "# --- Run on your eval_3_df ---\n",
        "eval_3_scored = add_logistic_score_3d(eval_3_df)\n",
        "\n",
        "y_true_3 = eval_3_scored[\"stroke_event\"].values\n",
        "y_prob_3 = eval_3_scored[\"p_3\"].values\n",
        "\n",
        "thr_df_3 = threshold_scan(y_true_3, y_prob_3, prevalence=0.10)\n",
        "\n",
        "# Example: best F1\n",
        "best_f1_row_3 = thr_df_3.loc[thr_df_3[\"f1\"].idxmax()]\n",
        "print(\"Best-F1 3d threshold row:\")\n",
        "print(best_f1_row_3)\n",
        "\n",
        "\n",
        "\n",
        "# AUCs for reporting\n",
        "auc_roc_3 = roc_auc_score(y_true_3, y_prob_3)\n",
        "auc_pr_3  = average_precision_score(y_true_3, y_prob_3)\n",
        "\n",
        "print(\"ROC AUC (3d):\", auc_roc_3)\n",
        "print(\"PR AUC (3d):\", auc_pr_3)\n"
      ],
      "metadata": {
        "id": "L9W4szSPZucH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose a specificity constraint\n",
        "SPEC_MIN = 0.90\n",
        "\n",
        "# restrict to high-specificity operating points\n",
        "thr_df_3_high_spec = thr_df_3[thr_df_3[\"specificity\"] >= SPEC_MIN]\n",
        "\n",
        "# guard against empty result\n",
        "if thr_df_3_high_spec.empty:\n",
        "    raise ValueError(\"No thresholds meet the specificity requirement.\")\n",
        "\n",
        "# best F1 under high specificity\n",
        "best_f1_high_spec_3 = thr_df_3_high_spec.loc[\n",
        "    thr_df_3_high_spec[\"f1\"].idxmax()\n",
        "]\n",
        "\n",
        "print(\"Best F1 with specificity ≥\", SPEC_MIN)\n",
        "print(best_f1_high_spec_3)\n"
      ],
      "metadata": {
        "id": "E7YtX1bwdckS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# symptom rule"
      ],
      "metadata": {
        "id": "QIDyqiOYFvac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def apply_symptom_rule_3d(\n",
        "    df: pd.DataFrame,\n",
        "    vh_thresh: int,\n",
        "    high_thresh: int,\n",
        "    mod_thresh: int,\n",
        "    hm_combo_thresh: int,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    3-day symptom rule with tunable integer thresholds.\n",
        "\n",
        "    Screen-positive if ANY are true:\n",
        "      1) VeryHigh_count_3d >= vh_thresh\n",
        "      2) High_count_3d >= high_thresh\n",
        "      3) High_count_3d >= 1 AND Moderate_count_3d >= mod_thresh\n",
        "      4) (High_count_3d + Moderate_count_3d) >= hm_combo_thresh\n",
        "    \"\"\"\n",
        "    needed = [\"VeryHigh_count_3d\", \"High_count_3d\", \"Moderate_count_3d\"]\n",
        "    missing = [c for c in needed if c not in df.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"Missing required columns for 3d rule: {missing}\")\n",
        "\n",
        "    vh = df[\"VeryHigh_count_3d\"]\n",
        "    h  = df[\"High_count_3d\"]\n",
        "    m  = df[\"Moderate_count_3d\"]\n",
        "\n",
        "    cond_vh = vh >= vh_thresh\n",
        "    cond_h  = h >= high_thresh\n",
        "    cond_hm_pair = (h >= 1) & (m >= mod_thresh)\n",
        "    cond_hm_sum  = (h + m) >= hm_combo_thresh\n",
        "\n",
        "    return (cond_vh | cond_h | cond_hm_pair | cond_hm_sum).astype(int).values\n",
        "\n",
        "\n",
        "def grid_search_symptom_rule_3d(\n",
        "    eval_3_df: pd.DataFrame,\n",
        "    vh_values=[1],\n",
        "    high_values=range(0, 4),\n",
        "    mod_values=range(0, 4),\n",
        "    hm_combo_values=range(1, 6),\n",
        "    prevalence: float | None = 0.10,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Grid search over thresholds for the 3-day symptom rule.\n",
        "    Returns tn/fp/fn/tp + metrics (+ optional prevalence-adjusted PPV/NPV).\n",
        "    \"\"\"\n",
        "    if \"stroke_event\" not in eval_3_df.columns:\n",
        "        raise KeyError(\"Missing required column 'stroke_event' in eval_3_df.\")\n",
        "\n",
        "    y_true = eval_3_df[\"stroke_event\"].astype(int).values\n",
        "    pi = None if prevalence is None else float(prevalence)\n",
        "\n",
        "    rows = []\n",
        "    for vh_t in vh_values:\n",
        "        for h_t in high_values:\n",
        "            for m_t in mod_values:\n",
        "                for hm_t in hm_combo_values:\n",
        "                    y_pred = apply_symptom_rule_3d(\n",
        "                        eval_3_df,\n",
        "                        vh_thresh=vh_t,\n",
        "                        high_thresh=h_t,\n",
        "                        mod_thresh=m_t,\n",
        "                        hm_combo_thresh=hm_t,\n",
        "                    )\n",
        "\n",
        "                    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "\n",
        "                    sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
        "                    spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
        "                    prec = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
        "                    npv  = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
        "                    f1   = (\n",
        "                        2 * prec * sens / (prec + sens)\n",
        "                        if not np.isnan(prec) and not np.isnan(sens) and (prec + sens) > 0\n",
        "                        else np.nan\n",
        "                    )\n",
        "                    acc  = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else np.nan\n",
        "\n",
        "                    out = {\n",
        "                        \"vh_thresh\": vh_t,\n",
        "                        \"high_thresh\": h_t,\n",
        "                        \"mod_thresh\": m_t,\n",
        "                        \"hm_combo_thresh\": hm_t,\n",
        "                        \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp,\n",
        "                        \"sensitivity\": sens,\n",
        "                        \"specificity\": spec,\n",
        "                        \"precision\": prec,\n",
        "                        \"npv\": npv,\n",
        "                        \"f1\": f1,\n",
        "                        \"accuracy\": acc,\n",
        "                    }\n",
        "\n",
        "                    #  prevalence-adjusted PPV/NPV\n",
        "                    if pi is not None and not (np.isnan(sens) or np.isnan(spec)):\n",
        "                        denom_ppv = sens * pi + (1 - spec) * (1 - pi)\n",
        "                        denom_npv = (1 - sens) * pi + spec * (1 - pi)\n",
        "                        out[\"ppv_adj\"] = (sens * pi) / denom_ppv if denom_ppv > 0 else np.nan\n",
        "                        out[\"npv_adj\"] = (spec * (1 - pi)) / denom_npv if denom_npv > 0 else np.nan\n",
        "                        out[\"assumed_prevalence\"] = pi\n",
        "                    else:\n",
        "                        out[\"ppv_adj\"] = np.nan\n",
        "                        out[\"npv_adj\"] = np.nan\n",
        "                        out[\"assumed_prevalence\"] = pi\n",
        "\n",
        "                    rows.append(out)\n",
        "\n",
        "    return pd.DataFrame(rows)\n"
      ],
      "metadata": {
        "id": "Phvw1ol9ZucH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_3d = grid_search_symptom_rule_3d(eval_3_df, prevalence=0.10)\n",
        "best_3 = grid_3d.loc[grid_3d[\"f1\"].idxmax()]\n",
        "best_3\n"
      ],
      "metadata": {
        "id": "bW9qOTMtZucH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a grid search for the optimal symptom based rule (3-day)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def apply_symptom_rule_3d(\n",
        "    df: pd.DataFrame,\n",
        "    vh_thresh: int,\n",
        "    high_thresh: int,\n",
        "    mod_thresh: int,\n",
        "    hm_combo_thresh: int,\n",
        ") -> np.ndarray:\n",
        "    vh = df[\"VeryHigh_count_3d\"]\n",
        "    h  = df[\"High_count_3d\"]\n",
        "    m  = df[\"Moderate_count_3d\"]\n",
        "\n",
        "    cond_vh = vh >= vh_thresh\n",
        "    cond_h  = h >= high_thresh\n",
        "    cond_hm_pair = (h >= 1) & (m >= mod_thresh)\n",
        "    cond_hm_sum  = (h + m) >= hm_combo_thresh\n",
        "\n",
        "    flag = cond_vh | cond_h | cond_hm_pair | cond_hm_sum\n",
        "    return flag.astype(int).values\n",
        "\n",
        "\n",
        "def grid_search_symptom_rule_3d(\n",
        "    eval_3_df: pd.DataFrame,\n",
        "    vh_values = [1],\n",
        "    high_values = range(0, 4),\n",
        "    mod_values = range(0, 4),\n",
        "    hm_combo_values = range(1, 6),\n",
        "    prevalence: float = 0.10,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with thresholds + tn/fp/fn/tp + metrics + F1\n",
        "    plus prevalence-adjusted PPV/NPV using a fixed prevalence.\n",
        "    \"\"\"\n",
        "    y_true = eval_3_df[\"stroke_event\"].astype(int).values\n",
        "    pi = float(prevalence)\n",
        "\n",
        "    rows = []\n",
        "    for vh_t in vh_values:\n",
        "        for h_t in high_values:\n",
        "            for m_t in mod_values:\n",
        "                for hm_t in hm_combo_values:\n",
        "                    y_pred = apply_symptom_rule_3d(\n",
        "                        eval_3_df,\n",
        "                        vh_thresh=vh_t,\n",
        "                        high_thresh=h_t,\n",
        "                        mod_thresh=m_t,\n",
        "                        hm_combo_thresh=hm_t,\n",
        "                    )\n",
        "\n",
        "                    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "\n",
        "                    sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
        "                    spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
        "                    prec = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
        "                    npv  = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
        "\n",
        "                    # --- prevalence-adjusted PPV/NPV (Bayes) ---\n",
        "                    if np.isnan(sens) or np.isnan(spec):\n",
        "                        ppv_adj = np.nan\n",
        "                        npv_adj = np.nan\n",
        "                    else:\n",
        "                        denom_ppv = sens * pi + (1 - spec) * (1 - pi)\n",
        "                        ppv_adj = (sens * pi) / denom_ppv if denom_ppv > 0 else np.nan\n",
        "\n",
        "                        denom_npv = (1 - sens) * pi + spec * (1 - pi)\n",
        "                        npv_adj = (spec * (1 - pi)) / denom_npv if denom_npv > 0 else np.nan\n",
        "\n",
        "                    f1 = (\n",
        "                        2 * prec * sens / (prec + sens)\n",
        "                        if not np.isnan(prec) and not np.isnan(sens) and (prec + sens) > 0\n",
        "                        else np.nan\n",
        "                    )\n",
        "                    acc = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else np.nan\n",
        "\n",
        "                    rows.append({\n",
        "                        \"vh_thresh\": vh_t,\n",
        "                        \"high_thresh\": h_t,\n",
        "                        \"mod_thresh\": m_t,\n",
        "                        \"hm_combo_thresh\": hm_t,\n",
        "                        \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp,\n",
        "                        \"sensitivity\": sens,\n",
        "                        \"specificity\": spec,\n",
        "                        \"precision\": prec,\n",
        "                        \"npv\": npv,\n",
        "                        \"ppv_adj_prev_0.10\": ppv_adj,\n",
        "                        \"npv_adj_prev_0.10\": npv_adj,\n",
        "                        \"f1\": f1,\n",
        "                        \"accuracy\": acc,\n",
        "                    })\n",
        "\n",
        "    return pd.DataFrame(rows)\n"
      ],
      "metadata": {
        "id": "z9HZGMh_ZucH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run grid search\n",
        "grid_3d = grid_search_symptom_rule_3d(eval_3_df)\n",
        "\n",
        "# Sort by F1 (descending)\n",
        "grid_3d_sorted = grid_3d.sort_values(\"f1\", ascending=False)\n",
        "\n",
        "# Top 100 combinations by F1\n",
        "grid_3d_sorted.head(100)"
      ],
      "metadata": {
        "id": "RvkpMY3_ZucH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# high specificity\n",
        "\n",
        "best_f1_high_spec = (\n",
        "    grid_3d\n",
        "    .query(\"specificity >= 0.9\")\n",
        "    .sort_values(\"f1\", ascending=False)\n",
        "    .iloc[0]\n",
        ")\n",
        "\n",
        "best_f1_high_spec\n"
      ],
      "metadata": {
        "id": "z75rI1snZucI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def symptom_rule_3d(\n",
        "    vh_count: int,\n",
        "    high_count: int,\n",
        "    mod_count: int,\n",
        "    vh_thresh: int,\n",
        "    high_thresh: int,\n",
        "    mod_thresh: int,\n",
        "    hm_combo_thresh: int,\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Row-wise 3-day symptom rule (used in two-stage screening).\n",
        "    \"\"\"\n",
        "\n",
        "    vh = int(vh_count)\n",
        "    h  = int(high_count)\n",
        "    m  = int(mod_count)\n",
        "\n",
        "    if vh >= vh_thresh:\n",
        "        return 1\n",
        "    if h >= high_thresh:\n",
        "        return 1\n",
        "    if h >= 1 and m >= mod_thresh:\n",
        "        return 1\n",
        "    if (h + m) >= hm_combo_thresh:\n",
        "        return 1\n",
        "\n",
        "    return 0\n"
      ],
      "metadata": {
        "id": "xn1LHuz7ZucI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply cleaned OR hybrid rule\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def apply_3day_hybrid_rule(\n",
        "    eval_3_scored: pd.DataFrame,\n",
        "    prob_threshold: float,\n",
        "    vh_thresh: int,\n",
        "    high_thresh: int,\n",
        "    mod_thresh: int,\n",
        "    hm_combo_thresh: int,\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    df = eval_3_scored.copy()\n",
        "\n",
        "    if \"p_3\" not in df.columns:\n",
        "        raise KeyError(\"Missing 'p_3'. Run add_logistic_score_3d first (or create p_3).\")\n",
        "\n",
        "    # ensure numeric counts, NaN -> 0\n",
        "    for c in [\"VeryHigh_count_3d\", \"High_count_3d\", \"Moderate_count_3d\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = 0\n",
        "\n",
        "    df[[\"VeryHigh_count_3d\", \"High_count_3d\", \"Moderate_count_3d\"]] = (\n",
        "        df[[\"VeryHigh_count_3d\", \"High_count_3d\", \"Moderate_count_3d\"]]\n",
        "          .apply(pd.to_numeric, errors=\"coerce\")\n",
        "          .fillna(0)\n",
        "    )\n",
        "\n",
        "    # logistic\n",
        "    df[\"logistic_flag_3d\"] = (df[\"p_3\"].astype(float) >= float(prob_threshold)).astype(int)\n",
        "\n",
        "    # symptom rule (row-wise)\n",
        "    df[\"symptom_flag_3d\"] = df.apply(\n",
        "        lambda r: symptom_rule_3d(\n",
        "            vh_count=r[\"VeryHigh_count_3d\"],\n",
        "            high_count=r[\"High_count_3d\"],\n",
        "            mod_count=r[\"Moderate_count_3d\"],\n",
        "            vh_thresh=vh_thresh,\n",
        "            high_thresh=high_thresh,\n",
        "            mod_thresh=mod_thresh,\n",
        "            hm_combo_thresh=hm_combo_thresh,\n",
        "        ),\n",
        "        axis=1,\n",
        "    ).astype(int)\n",
        "\n",
        "    # OR hybrid\n",
        "    df[\"hybrid_flag_3d\"] = ((df[\"symptom_flag_3d\"] == 1) | (df[\"logistic_flag_3d\"] == 1)).astype(int)\n",
        "\n",
        "    def label_3(r):\n",
        "        if r[\"symptom_flag_3d\"] == 1 and r[\"logistic_flag_3d\"] == 1:\n",
        "            return \"High-Risk Alert (3d, symptom+logistic)\"\n",
        "        elif r[\"symptom_flag_3d\"] == 1:\n",
        "            return \"High-Risk Alert (3d, symptom-rule)\"\n",
        "        elif r[\"logistic_flag_3d\"] == 1:\n",
        "            return \"High-Risk Alert (3d, logistic)\"\n",
        "        else:\n",
        "            return \"No Alert (3d)\"\n",
        "\n",
        "    df[\"hybrid_alert_3d\"] = df.apply(label_3, axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(y_true, y_pred, prevalence=0.10):\n",
        "    \"\"\"\n",
        "    Computes standard metrics + deployment-style (prevalence-adjusted) PPV/NPV.\n",
        "\n",
        "    prevalence: assumed real-world prevalence π (e.g., 0.10 = 10%)\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    y_pred = np.asarray(y_pred).astype(int)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "    n = tn + fp + fn + tp\n",
        "\n",
        "    prev_emp = (tp + fn) / n if n > 0 else np.nan\n",
        "    sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
        "    spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
        "    ppv  = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
        "    npv  = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
        "    f1   = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else np.nan\n",
        "    acc  = (tp + tn) / n if n > 0 else np.nan\n",
        "\n",
        "    # --- deployment-style PPV/NPV (Bayes with fixed prevalence π) ---\n",
        "    pi = float(prevalence)\n",
        "    if np.isnan(sens) or np.isnan(spec):\n",
        "        ppv_adj = np.nan\n",
        "        npv_adj = np.nan\n",
        "    else:\n",
        "        denom_ppv = sens * pi + (1 - spec) * (1 - pi)\n",
        "        ppv_adj = (sens * pi) / denom_ppv if denom_ppv > 0 else np.nan\n",
        "\n",
        "        denom_npv = (1 - sens) * pi + spec * (1 - pi)\n",
        "        npv_adj = (spec * (1 - pi)) / denom_npv if denom_npv > 0 else np.nan\n",
        "\n",
        "    return {\n",
        "        \"n\": n,\n",
        "        \"prevalence_empirical\": prev_emp,\n",
        "        \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp,\n",
        "        \"sensitivity\": sens,\n",
        "        \"specificity\": spec,\n",
        "        \"precision\": ppv,\n",
        "        \"npv\": npv,\n",
        "        \"ppv_adj_prev_0.10\": ppv_adj,\n",
        "        \"npv_adj_prev_0.10\": npv_adj,\n",
        "        \"f1\": f1,\n",
        "        \"accuracy\": acc,\n",
        "    }\n",
        "\n",
        "def scan_hybrid_3d_thresholds(\n",
        "    eval_3_scored: pd.DataFrame,\n",
        "    vh_thresh: int,\n",
        "    high_thresh: int,\n",
        "    mod_thresh: int,\n",
        "    hm_combo_thresh: int,\n",
        "    thresholds=None,\n",
        "    prevalence: float = 0.10,\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    if thresholds is None:\n",
        "        thresholds = np.linspace(0, 1, 101)\n",
        "\n",
        "    # compute symptom flag ONCE (speed)\n",
        "    base = apply_3day_hybrid_rule(\n",
        "        eval_3_scored,\n",
        "        prob_threshold=0.0,\n",
        "        vh_thresh=vh_thresh,\n",
        "        high_thresh=high_thresh,\n",
        "        mod_thresh=mod_thresh,\n",
        "        hm_combo_thresh=hm_combo_thresh,\n",
        "    )\n",
        "\n",
        "    y_true = base[\"stroke_event\"].astype(int).values\n",
        "    rows = []\n",
        "\n",
        "    for thr in thresholds:\n",
        "        logistic_flag = (base[\"p_3\"].astype(float) >= float(thr)).astype(int).values\n",
        "        hybrid_flag = ((base[\"symptom_flag_3d\"].values == 1) | (logistic_flag == 1)).astype(int)\n",
        "\n",
        "        met = compute_metrics(y_true, hybrid_flag, prevalence=prevalence)\n",
        "        met[\"threshold\"] = thr\n",
        "        rows.append(met)\n",
        "\n",
        "    return pd.DataFrame(rows)\n"
      ],
      "metadata": {
        "id": "b4cI5S5GZucI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyb_scan_3 = scan_hybrid_3d_thresholds(\n",
        "    eval_3_scored,\n",
        "    vh_thresh=vh_best,\n",
        "    high_thresh=h_best,\n",
        "    mod_thresh=m_best,\n",
        "    hm_combo_thresh=hm_best,\n",
        ")\n",
        "\n",
        "hyb_scan_3.head()\n"
      ],
      "metadata": {
        "id": "ld2nZrcFZucI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# high specificity\n",
        "\n",
        "screening_3 = (\n",
        "    hyb_scan_3\n",
        "    .query(\"specificity >= 0.9\")\n",
        "    .sort_values(\"sensitivity\", ascending=False)\n",
        ")\n",
        "\n",
        "screening_3.head(5)\n"
      ],
      "metadata": {
        "id": "m0XDbPfBZucI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alert burden\n",
        "\n",
        "hyb_scan_3.assign(\n",
        "    alert_rate=lambda d: (d[\"tp\"] + d[\"fp\"]) / d[\"n\"]\n",
        ")[[\"threshold\",\"sensitivity\",\"specificity\",\"ppv_adj_prev_0.10\",\"alert_rate\"]].head(50)\n"
      ],
      "metadata": {
        "id": "c5S-DNWiZucI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}