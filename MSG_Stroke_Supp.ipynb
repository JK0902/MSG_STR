{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Supp\n",
        "\n"
      ],
      "metadata": {
        "id": "GYlto7qidlq0"
      },
      "id": "GYlto7qidlq0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. Topic Mapping"
      ],
      "metadata": {
        "id": "TgFdUrMz2ZMr"
      },
      "id": "TgFdUrMz2ZMr"
    },
    {
      "cell_type": "code",
      "id": "LcKKQOAJSDYh7pm6L85AN1DV",
      "metadata": {
        "tags": [],
        "id": "LcKKQOAJSDYh7pm6L85AN1DV"
      },
      "source": [
        "\n",
        "# temperature: float = 0.0,\n",
        "# max_tokens: int = 2500\n",
        "# top_p=0.5,\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# LLM PROMPTS\n",
        "# -------------------------\n",
        "\n",
        "UPDATE_SYSTEM = \"\"\"\n",
        "You are updating a 3-level clinical taxonomy for de-identified clinical\n",
        "portal messages: MAIN -> SUB1 -> SUB2.\n",
        "\n",
        "You must:\n",
        "- **KEEP** existing categories as much as possible.\n",
        "- ADD new MAIN/SUB1/SUB2 categories to **CAPTURE NEW TOPICS**\n",
        "- **DO NOT merge/remove** unless those are almost identical.\n",
        "- Keep the taxonomy interpretable and moderately granular.\n",
        "- For symptom taxonomy, be **especially GRANULAR**.\n",
        "- In the REASONING line, only describe what you added, merged, or renamed.\n",
        "Do NOT include general statements like “kept existing structure intact.”\n",
        "\n",
        "\n",
        "Write your response in plain text.\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. BERTopic based validation"
      ],
      "metadata": {
        "id": "t2prZ9x9dP-r"
      },
      "id": "t2prZ9x9dP-r"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# =====================\n",
        "# Text normalization + seed keywords (safe: operates on labels, not patient text)\n",
        "# =====================\n",
        "STOP = {\"and\",\"or\",\"the\",\"of\",\"for\",\"to\",\"in\",\"on\",\"a\",\"an\",\"with\",\"without\"}\n",
        "\n",
        "def normalize_text(x: str) -> str:\n",
        "    x = \"\" if pd.isna(x) else str(x)\n",
        "    return re.sub(r\"\\s+\", \" \", x.strip())\n",
        "\n",
        "def phrase_to_keywords(phrase: str):\n",
        "    phrase = normalize_text(phrase).lower()\n",
        "    toks = re.findall(r\"[a-z]+\", phrase)\n",
        "    toks = [t for t in toks if t not in STOP and len(t) > 2]\n",
        "    return toks\n",
        "\n",
        "def build_seed_topic_list(seed_df: pd.DataFrame, main_col=\"main\", sub1_col=\"sub1\"):\n",
        "    #  use seed_df\n",
        "    if seed_df is None or len(seed_df) == 0:\n",
        "        raise ValueError(\"seed_df is empty. Provide a non-sensitive seed taxonomy (main/sub1).\")\n",
        "\n",
        "    tmp = seed_df[[main_col, sub1_col]].copy()\n",
        "    tmp[main_col] = tmp[main_col].map(normalize_text)\n",
        "    tmp[sub1_col] = tmp[sub1_col].map(normalize_text)\n",
        "    tmp = tmp.drop_duplicates(subset=[main_col, sub1_col])\n",
        "\n",
        "    grouped = tmp.groupby(main_col)[sub1_col].apply(list)\n",
        "\n",
        "    ontology_labels, seed_topic_list = [], []\n",
        "    for main_label, sub1_list in grouped.items():\n",
        "        main_kw = phrase_to_keywords(main_label)\n",
        "        sub_kw = []\n",
        "        for s in sub1_list:\n",
        "            sub_kw.extend(phrase_to_keywords(s))\n",
        "\n",
        "        seen, kws = set(), []\n",
        "        for w in (main_kw + sub_kw):\n",
        "            if w not in seen:\n",
        "                kws.append(w)\n",
        "                seen.add(w)\n",
        "\n",
        "        ontology_labels.append(main_label)\n",
        "        seed_topic_list.append(kws)\n",
        "\n",
        "    return ontology_labels, seed_topic_list, tmp\n",
        "\n",
        "# =====================\n",
        "# Embedding model\n",
        "# =====================\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def compute_embeddings(texts, embedder_name=\"all-MiniLM-L6-v2\", batch_size=128):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    embedder = SentenceTransformer(embedder_name, device=device)\n",
        "    emb = embedder.encode(\n",
        "        list(texts),\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=not SAFE_MODE,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=False,\n",
        "    )\n",
        "    return embedder, emb\n",
        "\n",
        "# =====================\n",
        "# BERTopic components\n",
        "# =====================\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "def fit_guided_bertopic(df: pd.DataFrame, seed_topic_list, embedder, embeddings):\n",
        "\n",
        "    vectorizer_model = CountVectorizer(\n",
        "        stop_words=\"english\",\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=2,\n",
        "        max_df=0.8,\n",
        "    )\n",
        "\n",
        "    # Dimensionality reduction + clustering\n",
        "    umap_model = UMAP(\n",
        "        n_neighbors=30,\n",
        "        n_components=10,\n",
        "        min_dist=0.0,\n",
        "        metric=\"cosine\",\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    hdbscan_model = HDBSCAN(\n",
        "        min_cluster_size=30,\n",
        "        min_samples=10,\n",
        "        prediction_data=True,\n",
        "    )\n",
        "\n",
        "    model = BERTopic(\n",
        "        embedding_model=embedder,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        representation_model=KeyBERTInspired(),\n",
        "        seed_topic_list=seed_topic_list,\n",
        "        top_n_words=10,\n",
        "        calculate_probabilities=True,\n",
        "        verbose=(not SAFE_MODE),\n",
        "        low_memory=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    topics, probs = model.fit_transform(df[TEXT_COL].tolist(), embeddings=embeddings)\n",
        "    info = model.get_topic_info()\n",
        "\n",
        "\n",
        "    if not SAFE_MODE:\n",
        "        _no_text_print(\"Non-outlier topics:\", (info[\"Topic\"] != -1).sum())\n",
        "        _no_text_print(info.sort_values(\"Count\", ascending=False).head(10)[[\"Topic\",\"Count\",\"Name\"]])\n",
        "\n",
        "    return model, topics, probs, info\n",
        "\n",
        "# =====================\n",
        "# Alignment: topic words -> ontology label\n",
        "# =====================\n",
        "def _cosine_sim_matrix(A, B):\n",
        "    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)\n",
        "    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)\n",
        "    return A @ B.T\n",
        "\n",
        "def align_topics_to_ontology_safer(model, embedder, ontology_labels):\n",
        "    \"\"\"\n",
        "    SAFER than encoding topic top-words strings:\n",
        "    - Uses BERTopic topic embeddings (centroids) if available\n",
        "    Returns a compact mapping table with no text.\n",
        "    \"\"\"\n",
        "    info = model.get_topic_info().copy()\n",
        "    info = info[info[\"Topic\"] != -1].reset_index(drop=True)\n",
        "    topic_ids = info[\"Topic\"].tolist()\n",
        "\n",
        "    topic_vecs = None\n",
        "    if hasattr(model, \"topic_embeddings_\") and model.topic_embeddings_ is not None:\n",
        "\n",
        "        topics_dict = model.get_topics()\n",
        "        ordered_topic_ids = [t for t in topics_dict.keys() if t != -1]\n",
        "        # Align embeddings to that ordering\n",
        "        if len(ordered_topic_ids) == len(model.topic_embeddings_):\n",
        "            id_to_vec = {tid: model.topic_embeddings_[i] for i, tid in enumerate(ordered_topic_ids)}\n",
        "            topic_vecs = np.vstack([id_to_vec[tid] for tid in topic_ids if tid in id_to_vec])\n",
        "\n",
        "    if topic_vecs is None:\n",
        "\n",
        "        topic_names = info[\"Name\"].astype(str).tolist()\n",
        "        topic_vecs = np.asarray(embedder.encode(topic_names, normalize_embeddings=True))\n",
        "\n",
        "    onto_vecs = np.asarray(embedder.encode(list(ontology_labels), normalize_embeddings=True))\n",
        "    sims = _cosine_sim_matrix(topic_vecs, onto_vecs)\n",
        "\n",
        "    best_idx = sims.argmax(axis=1)\n",
        "    best_sim = sims.max(axis=1)\n",
        "\n",
        "    aligned = pd.DataFrame({\n",
        "        \"topic_id\": topic_ids,\n",
        "        \"topic_size\": info[\"Count\"].tolist(),\n",
        "        \"main_label\": [ontology_labels[i] for i in best_idx],\n",
        "        \"alignment_score\": best_sim,\n",
        "    }).sort_values([\"main_label\", \"alignment_score\", \"topic_size\"], ascending=[True, False, False])\n",
        "\n",
        "    return aligned\n",
        "\n",
        "# =====================\n",
        "# \"main\" function (no I/O, no saves)\n",
        "# =====================\n",
        "def run_pipeline(df: pd.DataFrame, seed_df: pd.DataFrame):\n",
        "    _assert_safe_inputs(df)\n",
        "\n",
        "    ontology_labels, seed_topic_list, seed_dedup = build_seed_topic_list(seed_df)\n",
        "    embedder, embeddings = compute_embeddings(df[TEXT_COL].tolist())\n",
        "\n",
        "    model, topics, probs, info = fit_guided_bertopic(\n",
        "        df=df,\n",
        "        seed_topic_list=seed_topic_list,\n",
        "        embedder=embedder,\n",
        "        embeddings=embeddings,\n",
        "    )\n",
        "\n",
        "    aligned_topics = align_topics_to_ontology_safer(model, embedder, ontology_labels)\n",
        "\n",
        "    # Return only aggregate artifacts by default (no doc-level exports)\n",
        "    return {\n",
        "        \"topic_info\": info[[\"Topic\", \"Count\", \"Name\"]].copy(),\n",
        "        \"aligned_topics\": aligned_topics,\n",
        "        \"n_docs\": len(df),\n",
        "        \"n_topics\": int((info[\"Topic\"] != -1).sum()),\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BRNN3fvjdTq2"
      },
      "id": "BRNN3fvjdTq2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. Symptom_annotation"
      ],
      "metadata": {
        "id": "ThrB_fQW5RpV"
      },
      "id": "ThrB_fQW5RpV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### III. a. Initial labeling"
      ],
      "metadata": {
        "id": "bUNTqRiBDS7k"
      },
      "id": "bUNTqRiBDS7k"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "2CXnU0KblVn5ZMqclJ0KzhdG"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "import pandas as pd\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "# =========================\n",
        "# single-message classifier (Gemini)\n",
        "# =========================\n",
        "def gemini_model_fn(message: str,\n",
        "                   temperature: float = 0.0,\n",
        "                   max_tokens: int = 2500) -> Tuple[int, str]:\n",
        "    \"\"\"\n",
        "    Classify a single patient message into categories 0–11 and return (classification, reasoning).\n",
        "    \"\"\"\n",
        "\n",
        "    system_instruction = (\n",
        "        \"These are the messages from patients sent to healthcare professionals. \"\n",
        "        \"Rules: If a message contains both medical and non-medical issues, \"\n",
        "        \"prioritize addressing the medical topics (Symptom Updates & Clinical Concerns).\"\n",
        "    )\n",
        "\n",
        "    categories = (\n",
        "        \"Classify the message according to the following categories:\\n\"\n",
        "        \"1. Medication Issues; Medication Management and Refills; Requests for refills, Dosage adjustments, \"\n",
        "        \"Side Effects and safety concerns, Travel Medications, Compliance, Specific Types\\n\"\n",
        "        \"2. Symptom Updates & Clinical Concerns, including but not limited to diabetes, cardiovascular, pain, \"\n",
        "        \"musculoskeletal, neurological, injury, gastrointestinal, infectious disease, oncology, \"\n",
        "        \"urology/kidney, allergy/immunology, preventive vaccination/screening, respiratory, mental health, \"\n",
        "        \"sleep, ENT, Dermatology, lifestyle/diet, ophthalmology, endocrine, dental, hospitalization, \"\n",
        "        \"women's health\\n\"\n",
        "        \"3. Medical Equipment, Supplies, and Home Health\\n\"\n",
        "        \"4. Administrative Tasks (documents, forms, disability, insurance, etc.)\\n\"\n",
        "        \"5. Lab Test & Imaging\\n\"\n",
        "        \"6. Appointment Scheduling / Rescheduling / Cancelling\\n\"\n",
        "        \"7. Caregiver Support and Logistics\\n\"\n",
        "        \"8. Specialist Referral related issues\\n\"\n",
        "        \"9. General Communications (confirmation, gratitude)\\n\"\n",
        "        \"10. General Communications: non-medical/logistics (pharmacy, portal issues)\\n\"\n",
        "        \"11. General Communications (other)\\n\"\n",
        "        \"Otherwise 0.\\n\\n\"\n",
        "        \"Output format:\\n\"\n",
        "        \"Classification: <0 to 11>\\n\"\n",
        "        \"Reason: <reason_text>\"\n",
        "    )\n",
        "\n",
        "    full_prompt = (\n",
        "        f\"{system_instruction}\\n\\n\"\n",
        "        f\"{categories}\\n\"\n",
        "        f\"Message:\\n{message}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=full_prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=temperature,\n",
        "                max_output_tokens=max_tokens,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Safely extract the model output\n",
        "        content = extract_text_from_response(response)\n",
        "        if VERBOSE:\n",
        "            print(f\"[gemini_model_fn] Extracted content: {content}\", flush=True)\n",
        "\n",
        "        classification = 0\n",
        "        reasoning = \"No reason provided.\"\n",
        "\n",
        "        for line in content.splitlines():\n",
        "            if \"Classification:\" in line:\n",
        "                try:\n",
        "                    classification = int(line.split(\":\", 1)[1].strip())\n",
        "                except ValueError:\n",
        "                    classification = 0\n",
        "            elif \"Reason:\" in line:\n",
        "                reasoning = line.split(\":\", 1)[1].strip()\n",
        "\n",
        "        if VERBOSE:\n",
        "            print(f\"[gemini_model_fn] Parsed -> Classification: {classification}, Reason: {reasoning}\", flush=True)\n",
        "\n",
        "        # sanity bounds\n",
        "        if classification < 0 or classification > 11:\n",
        "            classification = 0\n",
        "\n",
        "        return classification, reasoning\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[gemini_model_fn] Error processing message: {e}\", flush=True)\n",
        "        return 0, \"Error processing message.\"\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2CXnU0KblVn5ZMqclJ0KzhdG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### III. b. Symptom labeling"
      ],
      "metadata": {
        "id": "sadLhKiuKTRA"
      },
      "id": "sadLhKiuKTRA"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Multi-label classifier (0–105)\n",
        "# =========================\n",
        "from typing import List, Tuple\n",
        "import re\n",
        "\n",
        "def gemini_model_fn(\n",
        "    message: str,\n",
        "    temperature: float = 0.0,\n",
        "    max_tokens: int = 2500,\n",
        ") -> Tuple[List[int], str]:\n",
        "    \"\"\"\n",
        "    Classify a single patient message into categories 0–105 and return\n",
        "    (list_of_classifications, reasoning).\n",
        "\n",
        "    Returns:\n",
        "        classifications: list[int] (up to 3 labels, each in [0, 105])\n",
        "        reasoning: str\n",
        "    \"\"\"\n",
        "\n",
        "    system_instruction = (\n",
        "        \"These are the messages from patients sent to healthcare professionals. \"\n",
        "        \"Rules: 1) If a message contains both medical and non-medical issues, prioritize medical topics.\\n\"\n",
        "        \"2) If, and only if, a message contains strictly non-medical topics, categorize it as 105.\\n\"\n",
        "        \"2) When categorizing, prefer the most relevant category available. \"\n",
        "        \"If the message contains two or more medical issues, prefer the top two most primary medical concerns. \"\n",
        "        \"If diabetes is mentioned, prefer other medical topics over diabetes.\\n\"\n",
        "        \"3) If, and only if, there are three equally important medical topics, you can categorize all three.\\n\"\n",
        "    )\n",
        "\n",
        "    categories = (\n",
        "        f\"1. Abdominal and flank pain, Abdominal lump or mass, Epigastric and stomach pain\\n\"\n",
        "        f\"2. Alcohol and substance use\\n\"\n",
        "        f\"3. Allergy/angioedema: Allergic swelling, Angioedema, EpiPen requests and usage\\n\"\n",
        "        f\"4. Anal/anorectal pain\\n\"\n",
        "        f\"5. Anticoagulation and clots: Anticoagulation management, Deep vein thrombosis (DVT) and clots\\n\"\n",
        "        f\"6. Anxiety-related: Anxiety symptoms, Anxiety medication inquiries\\n\"\n",
        "        f\"7. Arm/elbow pain\\n\"\n",
        "        f\"8. Arrhythmia and cardioversion updates; Atrial fibrillation (Afib) management\\n\"\n",
        "        f\"9. Arthritis flare\\n\"\n",
        "        f\"10. Asthma: Asthma management\\n\"\n",
        "        f\"11. Back pain\\n\"\n",
        "        f\"12. Bladder issues\\n\"\n",
        "        f\"13. Bloating and gas\\n\"\n",
        "        f\"14. Blood pressure and Managing hypo/hypertension, Medication-related BP effects\\n\"\n",
        "        f\"15. Bone pain\\n\"\n",
        "        f\"16. Breast: Breast symptoms\\n\"\n",
        "        f\"17. Cardiac rhythm and rate: Heart rate monitoring & control strategy; Palpitations\\n\"\n",
        "        f\"18. Cardiac testing: Cardiology testing (Holter, Echo), ECG / EKG inquiries and results\\n\"\n",
        "        f\"19. Care coordination: Admission and discharge coordination; ER admissions and updates\\n\"\n",
        "        f\"20. Chest congestion\\n\"\n",
        "        f\"21. Chills and fevers\\n\"\n",
        "        f\"22. Cholesterol and lipid management\\n\"\n",
        "        f\"23. Cold and flu symptoms\\n\"\n",
        "        f\"24. Colonoscopy and endoscopy inquiries\\n\"\n",
        "        f\"25. Concussion symptoms\\n\"\n",
        "        f\"26. Constipation\\n\"\n",
        "        f\"27. Cough\\n\"\n",
        "        f\"28. Coughing up blood (hemoptysis)\\n\"\n",
        "        f\"29. COVID-19 topics: COVID-19 testing and exposure, risk and safety, COVID-19 vaccination\\n\"\n",
        "        f\"30. Dementia and cognitive decline, Cognitive changes, Disorientation and confusion\\n\"\n",
        "        f\"31. Dental: Dental pain and concerns\\n\"\n",
        "        f\"32. Dermatology growths: Cysts and nodules, Moles and skin growths\\n\"\n",
        "        f\"33. Dermatology infections: Cellulitis and skin infections, Fungal infections\\n\"\n",
        "        f\"34. Dermatology reactions and lesions: Skin rashes and lesions, Skin reactions, Itching and pruritus, Dry skin\\n\"\n",
        "        f\"35. Diabetes monitoring and results: A1C goals and monitoring, Glucose readings, Glucose logs, Diabetes lab results, Diabetes education and coaching\\n\"\n",
        "        f\"36. Diabetes treatment and devices: Insulin management, Diabetes medication inquiries, Insulin pump & CGM device issues\\n\"\n",
        "        f\"37. Dialysis coordination, Post-dialysis symptoms\\n\"\n",
        "        f\"38. Diarrhea\\n\"\n",
        "        f\"39. Dizziness / vertigo\\n\"\n",
        "        f\"40. Dysuria, Hematuria\\n\"\n",
        "        f\"41. Ear issues: Hearing loss or changes\\n\"\n",
        "        f\"42. Endocrine/metabolic: Endocrine and thyroid concerns, Metabolic concerns\\n\"\n",
        "        f\"43. Eye pain or eye discharge\\n\"\n",
        "        f\"44. Foreign body or splinter, Cast and splint care\\n\"\n",
        "        f\"46. General body aches\\n\"\n",
        "        f\"47. General GI symptoms, Hernia concerns\\n\"\n",
        "        f\"48. Genital symptoms\\n\"\n",
        "        f\"49. GERD/esophageal: GERD and acid reflux, Esophageal pain\\n\"\n",
        "        f\"50. GI bleeding\\n\"\n",
        "        f\"51. Gout\\n\"\n",
        "        f\"52. Hand and wrist pain\\n\"\n",
        "        f\"53. Headaches and migraines\\n\"\n",
        "        f\"54. Hip pain\\n\"\n",
        "        f\"55. Hormone replacement therapy questions\\n\"\n",
        "        f\"56. Hyper and hypoglycemia episodes\\n\"\n",
        "        f\"57. Jaundice, Liver diagnosis and treatment\\n\"\n",
        "        f\"58. Jaw pain\\n\"\n",
        "        f\"59. Joint or muscle stiffness/pain/spasms/cramps\\n\"\n",
        "        f\"60. Knee and leg pain\\n\"\n",
        "        f\"61. Lifestyle and wellness: Diet and nutrition inquiries, Food logs, Exercise recommendations\\n\"\n",
        "        f\"62. Loss of appetite, Loss of taste or smell\\n\"\n",
        "        f\"63. Lymphedema\\n\"\n",
        "        f\"64. Malaise and fatigue\\n\"\n",
        "        f\"65. Mammogram inquiries\\n\"\n",
        "        f\"66. Mental health services: Therapy referrals, coordination or progress updates, Mental health concerns/referrals\\n\"\n",
        "        f\"67. Mobility and gait: Mobility and gait concerns\\n\"\n",
        "        f\"68. Mood-related: Depression symptoms, Mood updates\\n\"\n",
        "        f\"69. Nasal congestion and drainage\\n\"\n",
        "        f\"70. Nausea and vomiting\\n\"\n",
        "        f\"71. Neck or Shoulder pain or numbness\\n\"\n",
        "        f\"72. Neuropathy\\n\"\n",
        "        f\"73. Night sweats\\n\"\n",
        "        f\"74. Oncology care: Chemotherapy inquiries\\n\"\n",
        "        f\"75. Panic attacks\\n\"\n",
        "        f\"76. Pneumonia\\n\"\n",
        "        f\"77. Positional lightheadedness, Fainting and syncope\\n\"\n",
        "        f\"78. Prostate symptoms\\n\"\n",
        "        f\"79. Psychosis/behavior: Agitation and behavioral issues, Paranoia and hallucinations\\n\"\n",
        "        f\"80. Pulmonary fluid retention\\n\"\n",
        "        f\"81. Renal function and GFR concerns\\n\"\n",
        "        f\"82. Requests for opioids: Requests for opioids\\n\"\n",
        "        f\"83. Restless Leg Syndrome\\n\"\n",
        "        f\"84. Sciatica\\n\"\n",
        "        f\"85. Screening and preventive care: Annual physicals, General screening questions\\n\"\n",
        "        f\"86. Seizures and epilepsy\\n\"\n",
        "        f\"87. Shortness of breath (SOB), Wheezing\\n\"\n",
        "        f\"88. Sleep: Sleep apnea, Sleep disturbances, Insomnia, Narcolepsy, CPAP usage and compliance, Melatonin inquiries\\n\"\n",
        "        f\"89. Stool color or consistency changes\\n\"\n",
        "        f\"90. Surgery decisions: Cardiac surgery planning, Surgical alternatives\\n\"\n",
        "        f\"91. TB testing or related\\n\"\n",
        "        f\"92. Throat issues: Throat irritation, Parotitis and gland swelling\\n\"\n",
        "        f\"93. Toes/Foot/ankle pain/swelling, Heel spurs\\n\"\n",
        "        f\"94. Trauma and injury: Falls, Trauma, and physical injury, Fractures, Bruising and soft tissue injury\\n\"\n",
        "        f\"95. Tremors, Involuntary movements\\n\"\n",
        "        f\"96. Unintentional weight loss\\n\"\n",
        "        f\"97. Urinary Tract Infection (UTI)\\n\"\n",
        "        f\"98. Urinary urgency and retention, Incontinence\\n\"\n",
        "        f\"99. Vaccines and immunizations: Vaccines and immunizations: availability and side effects, Travel consultation and vaccines\\n\"\n",
        "        f\"100. Vaginal yeast infection\\n\"\n",
        "        f\"101. Vision changes or Retina exam\\n\"\n",
        "        f\"102. Vitamin and supplement inquiries, Probiotics and gut health\\n\"\n",
        "        f\"103. Weight reporting\\n\"\n",
        "        f\"104. Wounds and ulcers: Blisters, Skin ulcers, Wound care, Topical medication questions\\n\"\n",
        "        f\"105. Non-Medical Issues\\n\"\n",
        "        \"Otherwise 0, and provide the reasoning.\\n\"\n",
        "        \"Output format:\\n\"\n",
        "        \"Classification: <0 to 105>\\n\"\n",
        "        \"Reason: <reason_text>\"\n",
        "    )\n",
        "\n",
        "    full_prompt = (\n",
        "        f\"{system_instruction}\\n\\n\"\n",
        "        f\"{categories}\\n\"\n",
        "        f\"Message:\\n{message}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=full_prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=temperature,\n",
        "                max_output_tokens=max_tokens,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        content = extract_text_from_response(response)\n",
        "        if VERBOSE:\n",
        "            print(f\"[gemini_model_fn] extracted {len(content)} chars\", flush=True)\n",
        "\n",
        "\n",
        "        classifications: List[int] = [0]\n",
        "        reasoning = \"No reason provided.\"\n",
        "\n",
        "        for line in content.splitlines():\n",
        "            line_stripped = line.strip()\n",
        "\n",
        "            if line_stripped.lower().startswith(\"classification\"):\n",
        "                cls_part = line_stripped.split(\":\", 1)[1]\n",
        "\n",
        "                # Extract all integers: handles \"14, 59, 35\" etc.\n",
        "                nums = [int(n) for n in re.findall(r\"\\d+\", cls_part)]\n",
        "\n",
        "                # Clean: keep unique, 0–105 only, max 3\n",
        "                clean: List[int] = []\n",
        "                for n in nums:\n",
        "                    if 0 <= n <= 105 and n not in clean:\n",
        "                        clean.append(n)\n",
        "                    if len(clean) == 3:\n",
        "                        break\n",
        "\n",
        "                classifications = clean or [0]\n",
        "\n",
        "            elif line_stripped.lower().startswith(\"reason\"):\n",
        "                reasoning = line_stripped.split(\":\", 1)[1].strip()\n",
        "\n",
        "        if VERBOSE:\n",
        "            print(\n",
        "                f\"[gemini_model_fn] Parsed -> Classifications: {classifications}, Reason: {reasoning}\",\n",
        "                flush=True,\n",
        "            )\n",
        "\n",
        "        # Extra safety: enforce range and ensure not empty\n",
        "        classifications = [c for c in classifications if 0 <= c <= 105] or [0]\n",
        "\n",
        "        return classifications, reasoning\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[gemini_model_fn] Error processing message: {e}\", flush=True)\n",
        "        return [0], \"Error processing message.\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aqgqpfcrGZgI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aqgqpfcrGZgI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV. Dual ML pipeline"
      ],
      "metadata": {
        "id": "MllHBPSv9Jt2"
      },
      "id": "MllHBPSv9Jt2"
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# Main (EXPORT-FREE / PRINT-ONLY)\n",
        "# =======================\n",
        "if __name__ == \"__main__\":\n",
        "    # Load tables\n",
        "    messages = load_messages(MESSAGES_CSV)\n",
        "    symptoms = load_symptoms(SYMPTOMS_CSV)\n",
        "\n",
        "    # Ensure unique node IDs\n",
        "    messages = messages.drop_duplicates(subset=\"m_id\", keep=\"last\").copy()\n",
        "    symptoms = symptoms.drop_duplicates(subset=\"symptom_id\", keep=\"last\").copy()\n",
        "\n",
        "    # Limit columns to avoid accidental prints/leaks downstream\n",
        "    # (we still need p_id, m_id, text, timestamp)\n",
        "    keep_msg_cols = [c for c in [\"p_id\", \"m_id\", \"text\", \"timestamp\"] if c in messages.columns]\n",
        "    messages = messages[keep_msg_cols].copy()\n",
        "\n",
        "    p_ids_from_msgs = messages[\"p_id\"].astype(str).unique().tolist()\n",
        "\n",
        "    # Persons: demographics + comorbids\n",
        "    person_demo, demo_cols = load_person_demo(PERSON_DEMO_CSV, p_ids=p_ids_from_msgs, one_hot=True)\n",
        "    comorbids, icd_cols = load_comorbids(\n",
        "        COMORBID_CSV, p_ids=p_ids_from_msgs,\n",
        "        min_patients_per_code=ICD_MIN_PREV, max_codes=ICD_MAX_COLS\n",
        "    )\n",
        "\n",
        "    persons = person_demo.merge(comorbids, on=\"p_id\", how=\"left\").fillna(0)\n",
        "    person_feature_cols = demo_cols + icd_cols\n",
        "    persons = persons.drop_duplicates(subset=\"p_id\", keep=\"last\").copy()\n",
        "\n",
        "    # Temporal person features\n",
        "    persons, person_feature_cols = add_temporal_person_features(messages, persons, person_feature_cols)\n",
        "\n",
        "    # Build message→symptom edges\n",
        "    msg_emb = sym_emb = None\n",
        "    if USE_LLM_SCORES and LLM_SCORES_PATH.exists():\n",
        "        print(f\"[INFO] Using LLM scores at: {LLM_SCORES_PATH}\")\n",
        "        ms_edges = message_to_symptom_edges_from_llm(\n",
        "            messages, symptoms,\n",
        "            llm_scores_path=LLM_SCORES_PATH,\n",
        "            score_col=LLM_SCORE_COLUMN,\n",
        "            is_proba=LLM_IS_PROBA,\n",
        "            min_score=EDGE_MIN_SIM,\n",
        "            top_k=TOP_K_SYMPTOMS,\n",
        "        )\n",
        "\n",
        "        # Optional: precompute embeddings (only in memory)\n",
        "        msg_emb = embed_texts(messages[\"text\"].fillna(\"\").astype(str).tolist())\n",
        "        sym_emb = embed_texts(\n",
        "            (symptoms[\"symptom_name\"].astype(str) + \": \" + symptoms[\"description\"].astype(str)).tolist()\n",
        "        )\n",
        "\n",
        "        if BLEND_WITH_EMBEDDINGS:\n",
        "            emb_edges, _, _ = message_to_symptom_edges_via_embeddings(\n",
        "                messages, symptoms, top_k=TOP_K_SYMPTOMS, min_sim=EDGE_MIN_SIM\n",
        "            )\n",
        "            mix = ms_edges.merge(emb_edges, on=[\"m_id\", \"symptom_id\"], how=\"outer\", suffixes=(\"_llm\", \"_emb\")).fillna(0.0)\n",
        "            mix[\"weight\"] = BLEND_ALPHA * mix[\"weight_llm\"] + (1.0 - BLEND_ALPHA) * mix[\"weight_emb\"]\n",
        "            ms_edges = mix[[\"m_id\", \"symptom_id\", \"weight\"]]\n",
        "    else:\n",
        "        print(\"[INFO] LLM scores not found → using embeddings only.\")\n",
        "        ms_edges, msg_emb, sym_emb = message_to_symptom_edges_via_embeddings(\n",
        "            messages, symptoms, top_k=TOP_K_SYMPTOMS, min_sim=EDGE_MIN_SIM\n",
        "        )\n",
        "\n",
        "    # Keep edges that point to surviving nodes (AFTER ms_edges exists)\n",
        "    ms_edges = ms_edges[\n",
        "        ms_edges[\"m_id\"].astype(str).isin(messages[\"m_id\"].astype(str)) &\n",
        "        ms_edges[\"symptom_id\"].astype(str).isin(symptoms[\"symptom_id\"].astype(str))\n",
        "    ].copy()\n",
        "\n",
        "    # Negation down-weighting\n",
        "    ms_edges = apply_negation_downweight(messages, symptoms, ms_edges)\n",
        "\n",
        "    # Basic aggregate diagnostics (safe)\n",
        "    print(f\"[INFO] messages: n={len(messages):,}\")\n",
        "    print(f\"[INFO] symptoms: n={len(symptoms):,}\")\n",
        "    print(f\"[INFO] persons:  n={persons['p_id'].nunique():,}\")\n",
        "    print(f\"[INFO] ms_edges: n={len(ms_edges):,} (avg edges/msg ≈ {len(ms_edges)/max(len(messages),1):.2f})\")\n",
        "\n",
        "    # If PyG unavailable, do EN baseline in-memory + print only\n",
        "    if not _PYG_OK:\n",
        "        print(\"[WARN] torch_geometric not installed → skipping GNN.\")\n",
        "        X_sym = (ms_edges.merge(messages[[\"m_id\", \"p_id\"]], on=\"m_id\", how=\"left\")\n",
        "                        .pivot_table(index=\"p_id\", columns=\"symptom_id\", values=\"weight\", aggfunc=\"sum\", fill_value=0.0))\n",
        "        print(f\"[INFO] Built person×symptom matrix in-memory: shape={X_sym.shape} (NOT EXPORTED)\")\n",
        "\n",
        "        if RUN_EN_BASELINE_QUICK:\n",
        "            coefs, cv_auc = elastic_net_baseline(persons, X_sym, l1_ratio=EN_L1_RATIO, C=EN_C, cv=EN_CV)\n",
        "            print(f\"Elastic Net CV AUC (quick): {cv_auc:.3f}\")\n",
        "\n",
        "            top = coefs.abs().sort_values(ascending=False).head(20).rename(\"abs_coef\").reset_index()\n",
        "            top = top.rename(columns={\"index\": \"symptom_id\"}).merge(\n",
        "                symptoms[[\"symptom_id\", \"symptom_name\"]], on=\"symptom_id\", how=\"left\"\n",
        "            )\n",
        "            print(\"\\nTop 20 EN |coef| symptoms (aggregate):\")\n",
        "            print(top[[\"symptom_id\", \"symptom_name\", \"abs_coef\"]].to_string(index=False))\n",
        "        else:\n",
        "            print(\"[INFO] Quick EN baseline skipped.\")\n",
        "        raise SystemExit(0)\n",
        "\n",
        "    # Build base graph\n",
        "    hetero, id_maps = build_hetero_graph(\n",
        "        messages, ms_edges, symptoms,\n",
        "        persons_df=persons, person_feature_cols=person_feature_cols,\n",
        "        msg_emb=msg_emb, sym_emb=sym_emb,\n",
        "        add_message_knn=True, k_msg_sim=5, add_symptom_cooc=True,\n",
        "    )\n",
        "\n",
        "    # Build dx pairs from comorbid indicator matrix\n",
        "    if icd_cols:\n",
        "        dx_pairs = (persons[[\"p_id\"] + icd_cols]\n",
        "                    .melt(id_vars=[\"p_id\"], var_name=\"dx_id\", value_name=\"has_code\"))\n",
        "        dx_pairs = dx_pairs.loc[dx_pairs[\"has_code\"] > 0, [\"p_id\", \"dx_id\"]].copy()\n",
        "        hetero, id_maps = add_dx_to_graph(hetero, dx_pairs, id_maps)\n",
        "\n",
        "    # Person similarity edges\n",
        "    hetero = add_person_similarity_edges_safe(hetero, persons, person_feature_cols, id_maps, k=5, min_sim=0.25)\n",
        "\n",
        "    assert_graph_ok(hetero, id_maps)\n",
        "\n",
        "    # Train GNN\n",
        "    model, z, person_probs = train_gnn(hetero, persons, id_maps)\n",
        "\n",
        "    # PRINT-ONLY person_probs aggregate summary (no IDs)\n",
        "    person_probs = np.asarray(person_probs, dtype=float)\n",
        "    print(f\"[INFO] GNN person_probs (NOT EXPORTED): n={person_probs.size:,} \"\n",
        "          f\"mean={person_probs.mean():.4f} sd={person_probs.std():.4f} \"\n",
        "          f\"p50={np.quantile(person_probs, 0.50):.4f} p95={np.quantile(person_probs, 0.95):.4f}\")\n",
        "\n",
        "    # Symptom importance + EN baseline (computed in-memory)\n",
        "    sym_rank = rank_symptoms(messages, ms_edges, hetero, z, id_maps).merge(symptoms, on=\"symptom_id\", how=\"left\")\n",
        "    X_sym = build_person_symptom_matrix(messages, ms_edges, id_maps)\n",
        "\n",
        "    if RUN_EN_BASELINE_QUICK:\n",
        "        coefs, cv_auc = elastic_net_baseline(persons, X_sym, l1_ratio=EN_L1_RATIO, C=EN_C, cv=EN_CV)\n",
        "        print(f\"Elastic Net CV AUC (quick): {cv_auc:.3f}\")\n",
        "\n",
        "        coef_df = coefs.rename(\"elasticnet_coef\").reset_index().rename(columns={\"index\": \"symptom_id\"})\n",
        "        merged = sym_rank.merge(coef_df, on=\"symptom_id\", how=\"left\")\n",
        "        merged[\"elasticnet_coef_abs\"] = merged[\"elasticnet_coef\"].abs()\n",
        "\n",
        "        pipe_en, X_en, y_en = elastic_net_fit(persons, X_sym, l1_ratio=EN_L1_RATIO, C=EN_C)\n",
        "        perm_imp = elastic_net_permutation_importance(\n",
        "            pipe_en, X_en, y_en, feature_names=X_sym.columns.tolist(), n_repeats=10\n",
        "        )\n",
        "        perm_df = perm_imp.rename(\"en_perm_importance\").reset_index().rename(columns={\"index\": \"symptom_id\"})\n",
        "        merged = merged.merge(perm_df, on=\"symptom_id\", how=\"left\")\n",
        "\n",
        "        def _z(col):\n",
        "            arr = merged[col].fillna(0.0).values\n",
        "            mu, sd = arr.mean(), arr.std() + 1e-9\n",
        "            return (arr - mu) / sd\n",
        "\n",
        "        merged[\"event_assoc_score\"] = _z(\"en_perm_importance\")\n",
        "    else:\n",
        "        merged = sym_rank.copy()\n",
        "        for col in [\"elasticnet_coef\", \"elasticnet_coef_abs\", \"en_perm_importance\", \"event_assoc_score\"]:\n",
        "            merged[col] = np.nan\n",
        "        print(\"[INFO] Quick EN baseline skipped.\")\n",
        "\n",
        "    # Print top symptoms (aggregate only; no text, no IDs beyond symptom_id)\n",
        "    cols_show = [\n",
        "        \"symptom_id\", \"symptom_name\",\n",
        "        \"coverage\", \"coverage_recency\",\n",
        "        \"importance_score\",\n",
        "        \"en_perm_importance\",\n",
        "        \"event_assoc_score\",\n",
        "        \"elasticnet_coef\"\n",
        "    ]\n",
        "    cols_show = [c for c in cols_show if c in merged.columns]\n",
        "\n",
        "    top20 = (merged.sort_values(\"importance_score\", ascending=False)\n",
        "                   .loc[:, cols_show]\n",
        "                   .head(20)\n",
        "                   .fillna(0.0))\n",
        "\n",
        "    print(\"\\nTop 20 symptoms (aggregate only):\")\n",
        "    print(top20.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "ToV08MTM88tg"
      },
      "id": "ToV08MTM88tg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pa2x3la3fwGJ"
      },
      "id": "pa2x3la3fwGJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Redaction-safe logging\n",
        "# =====================\n",
        "def _log(msg: str) -> None:\n",
        "    # Only aggregate, non-sensitive logs in SAFE_MODE\n",
        "    if SAFE_MODE:\n",
        "        print(msg, flush=True)\n",
        "    else:\n",
        "        print(msg, flush=True)\n",
        "\n",
        "def _log_debug(msg: str) -> None:\n",
        "    if (not SAFE_MODE) and VERBOSE:\n",
        "        print(msg, flush=True)\n",
        "\n",
        "def _raise_if_public_run_on_text() -> None:\n",
        "    if SAFE_MODE and not ALLOW_TEXT_INPUT:\n",
        "        raise RuntimeError(\n",
        "            \"SAFE_MODE is ON and ALLOW_TEXT_INPUT is OFF.\\n\"\n",
        "            \"This public-safe script refuses to process real text by default.\\n\"\n",
        "            \"Flip ALLOW_TEXT_INPUT=True only in an approved secure environment.\"\n",
        "        )\n",
        "\n",
        "def _assert_columns(df: pd.DataFrame, required: List[str], name: str = \"df\") -> None:\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"{name} missing required columns: {missing}\")\n",
        "\n",
        "\n",
        "# =====================\n",
        "# I. Topic Mapping (prompt template only)\n",
        "# =====================\n",
        "UPDATE_SYSTEM = \"\"\"\n",
        "You are updating a 3-level clinical taxonomy for de-identified clinical\n",
        "portal messages: MAIN -> SUB1 -> SUB2.\n",
        "\n",
        "You must:\n",
        "- **KEEP** existing categories as much as possible.\n",
        "- ADD new MAIN/SUB1/SUB2 categories to **CAPTURE NEW TOPICS**\n",
        "- **DO NOT merge/remove** unless those are almost identical.\n",
        "- Keep the taxonomy interpretable and moderately granular.\n",
        "- For symptom taxonomy, be **especially GRANULAR**.\n",
        "- In the REASONING line, only describe what you added, merged, or renamed.\n",
        "Do NOT include general statements like “kept existing structure intact.”\n",
        "\n",
        "Write your response in plain text.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# =====================\n",
        "# II. BERTopic-based validation\n",
        "# =====================\n",
        "\n",
        "STOP = {\"and\",\"or\",\"the\",\"of\",\"for\",\"to\",\"in\",\"on\",\"a\",\"an\",\"with\",\"without\"}\n",
        "\n",
        "def normalize_text(x: str) -> str:\n",
        "    x = \"\" if pd.isna(x) else str(x)\n",
        "    return re.sub(r\"\\s+\", \" \", x.strip())\n",
        "\n",
        "def phrase_to_keywords(phrase: str) -> List[str]:\n",
        "    phrase = normalize_text(phrase).lower()\n",
        "    toks = re.findall(r\"[a-z]+\", phrase)\n",
        "    toks = [t for t in toks if t not in STOP and len(t) > 2]\n",
        "    return toks\n",
        "\n",
        "def build_seed_topic_list(seed_df: pd.DataFrame, main_col: str = \"main\", sub1_col: str = \"sub1\"):\n",
        "    \"\"\"\n",
        "    seed_df should be non-sensitive / publishable (or synthetic).\n",
        "    \"\"\"\n",
        "    if seed_df is None or len(seed_df) == 0:\n",
        "        raise ValueError(\"seed_df is empty. Provide a non-sensitive seed taxonomy (main/sub1).\")\n",
        "\n",
        "    _assert_columns(seed_df, [main_col, sub1_col], name=\"seed_df\")\n",
        "\n",
        "    tmp = seed_df[[main_col, sub1_col]].copy()\n",
        "    tmp[main_col] = tmp[main_col].map(normalize_text)\n",
        "    tmp[sub1_col] = tmp[sub1_col].map(normalize_text)\n",
        "    tmp = tmp.drop_duplicates(subset=[main_col, sub1_col])\n",
        "\n",
        "    grouped = tmp.groupby(main_col)[sub1_col].apply(list)\n",
        "\n",
        "    ontology_labels: List[str] = []\n",
        "    seed_topic_list: List[List[str]] = []\n",
        "\n",
        "    for main_label, sub1_list in grouped.items():\n",
        "        main_kw = phrase_to_keywords(main_label)\n",
        "        sub_kw: List[str] = []\n",
        "        for s in sub1_list:\n",
        "            sub_kw.extend(phrase_to_keywords(s))\n",
        "\n",
        "        seen, kws = set(), []\n",
        "        for w in (main_kw + sub_kw):\n",
        "            if w not in seen:\n",
        "                kws.append(w)\n",
        "                seen.add(w)\n",
        "\n",
        "        ontology_labels.append(main_label)\n",
        "        seed_topic_list.append(kws)\n",
        "\n",
        "    return ontology_labels, seed_topic_list, tmp\n",
        "\n",
        "\n",
        "# ---- embeddings (in-memory only) ----\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def compute_embeddings(texts: List[str], embedder_name: str = \"all-MiniLM-L6-v2\", batch_size: int = 128):\n",
        "    _raise_if_public_run_on_text()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    embedder = SentenceTransformer(embedder_name, device=device)\n",
        "    emb = embedder.encode(\n",
        "        list(texts),\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=(not SAFE_MODE),  # reduce chatter in public runs\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=False,\n",
        "    )\n",
        "    return embedder, emb\n",
        "\n",
        "\n",
        "# ---- BERTopic ----\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "def fit_guided_bertopic(df: pd.DataFrame, seed_topic_list, embedder, embeddings):\n",
        "    _raise_if_public_run_on_text()\n",
        "    _assert_columns(df, [ID_COL, TEXT_COL], name=\"df\")\n",
        "\n",
        "    vectorizer_model = CountVectorizer(\n",
        "        stop_words=\"english\",\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=2,\n",
        "        max_df=0.8,\n",
        "    )\n",
        "\n",
        "    umap_model = UMAP(\n",
        "        n_neighbors=30,\n",
        "        n_components=10,\n",
        "        min_dist=0.0,\n",
        "        metric=\"cosine\",\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    hdbscan_model = HDBSCAN(\n",
        "        min_cluster_size=30,\n",
        "        min_samples=10,\n",
        "        prediction_data=True,\n",
        "    )\n",
        "\n",
        "    model = BERTopic(\n",
        "        embedding_model=embedder,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        representation_model=KeyBERTInspired(),\n",
        "        seed_topic_list=seed_topic_list,\n",
        "        top_n_words=10,\n",
        "        calculate_probabilities=True,\n",
        "        verbose=False,\n",
        "        low_memory=True,\n",
        "    )\n",
        "\n",
        "    topics, probs = model.fit_transform(df[TEXT_COL].astype(str).tolist(), embeddings=embeddings)\n",
        "    info = model.get_topic_info()\n",
        "\n",
        "    # SAFE_MODE: never print topic names/words\n",
        "    _log(f\"[INFO] BERTopic fit complete: n_docs={len(df):,}, n_topics={(info['Topic']!=-1).sum():,}, outliers={(info['Topic']==-1).sum():,}\")\n",
        "    return model, topics, probs, info\n",
        "\n",
        "\n",
        "def _cosine_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)\n",
        "    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)\n",
        "    return A @ B.T\n",
        "\n",
        "def align_topics_to_ontology_safer(model, embedder, ontology_labels: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    SAFER alignment:\n",
        "    - Prefer topic centroid embeddings if available\n",
        "    - Otherwise use topic \"Name\" embeddings as fallback (still derived from corpus)\n",
        "    In SAFE_MODE, we will NOT export or print the Name strings.\n",
        "    \"\"\"\n",
        "    info = model.get_topic_info().copy()\n",
        "    info = info[info[\"Topic\"] != -1].reset_index(drop=True)\n",
        "    topic_ids = info[\"Topic\"].tolist()\n",
        "\n",
        "    topic_vecs = None\n",
        "\n",
        "    # Try BERTopic topic centroids\n",
        "    if hasattr(model, \"topic_embeddings_\") and model.topic_embeddings_ is not None:\n",
        "        topics_dict = model.get_topics()\n",
        "        ordered_topic_ids = [t for t in topics_dict.keys() if t != -1]\n",
        "        if len(ordered_topic_ids) == len(model.topic_embeddings_):\n",
        "            id_to_vec = {tid: model.topic_embeddings_[i] for i, tid in enumerate(ordered_topic_ids)}\n",
        "            vecs = []\n",
        "            for tid in topic_ids:\n",
        "                if tid in id_to_vec:\n",
        "                    vecs.append(id_to_vec[tid])\n",
        "            if len(vecs) == len(topic_ids):\n",
        "                topic_vecs = np.vstack(vecs)\n",
        "\n",
        "    # Fallback: encode topic Name (avoid returning the raw strings)\n",
        "    if topic_vecs is None:\n",
        "        names = info[\"Name\"].astype(str).tolist()\n",
        "        topic_vecs = np.asarray(embedder.encode(names, normalize_embeddings=True))\n",
        "\n",
        "    onto_vecs = np.asarray(embedder.encode(list(ontology_labels), normalize_embeddings=True))\n",
        "    sims = _cosine_sim_matrix(topic_vecs, onto_vecs)\n",
        "\n",
        "    best_idx = sims.argmax(axis=1)\n",
        "    best_sim = sims.max(axis=1)\n",
        "\n",
        "    aligned = pd.DataFrame({\n",
        "        \"topic_id\": topic_ids,\n",
        "        \"topic_size\": info[\"Count\"].tolist(),\n",
        "        \"main_label\": [ontology_labels[i] for i in best_idx],\n",
        "        \"alignment_score\": best_sim,\n",
        "    }).sort_values([\"main_label\", \"alignment_score\", \"topic_size\"], ascending=[True, False, False])\n",
        "\n",
        "    return aligned\n",
        "\n",
        "\n",
        "def run_bertopic_validation(df: pd.DataFrame, seed_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Returns aggregate artifacts only.\n",
        "    SAFE_MODE: does NOT return topic Name strings.\n",
        "    \"\"\"\n",
        "    _assert_columns(df, [ID_COL, TEXT_COL], name=\"df\")\n",
        "    _raise_if_public_run_on_text()\n",
        "\n",
        "    ontology_labels, seed_topic_list, seed_dedup = build_seed_topic_list(seed_df)\n",
        "    embedder, embeddings = compute_embeddings(df[TEXT_COL].astype(str).tolist())\n",
        "\n",
        "    model, topics, probs, info = fit_guided_bertopic(df, seed_topic_list, embedder, embeddings)\n",
        "    aligned_topics = align_topics_to_ontology_safer(model, embedder, ontology_labels)\n",
        "\n",
        "    # Drop Name in SAFE_MODE (topic names can leak)\n",
        "    if SAFE_MODE:\n",
        "        topic_info = info[[\"Topic\", \"Count\"]].copy()\n",
        "    else:\n",
        "        topic_info = info[[\"Topic\", \"Count\", \"Name\"]].copy()\n",
        "\n",
        "    return {\n",
        "        \"topic_info\": topic_info,\n",
        "        \"aligned_topics\": aligned_topics,\n",
        "        \"n_docs\": int(len(df)),\n",
        "        \"n_topics\": int((info[\"Topic\"] != -1).sum()),\n",
        "    }\n",
        "\n",
        "\n",
        "# =====================\n",
        "# III. Symptom annotation (Gemini) — safe wrappers\n",
        "# =====================\n",
        "\n",
        "try:\n",
        "    from google import genai\n",
        "    from google.genai import types\n",
        "except Exception:\n",
        "    genai = None\n",
        "    types = None\n",
        "\n",
        "\n",
        "def gemini_single_label(\n",
        "    client,\n",
        "    MODEL_ID: str,\n",
        "    message: str,\n",
        "    temperature: float = 0.0,\n",
        "    max_tokens: int = 2500,\n",
        ") -> Tuple[int, str]:\n",
        "    \"\"\"\n",
        "    Classify a single message into categories 0–11.\n",
        "    SAFE_MODE: never prints model output; truncates returned rationale.\n",
        "    \"\"\"\n",
        "    _require_llm_enabled()\n",
        "    _raise_if_public_run_on_text()\n",
        "\n",
        "    system_instruction = (\n",
        "        \"These are the messages from patients sent to healthcare professionals. \"\n",
        "        \"Rules: If a message contains both medical and non-medical issues, \"\n",
        "        \"prioritize addressing the medical topics (Symptom Updates & Clinical Concerns).\"\n",
        "    )\n",
        "\n",
        "    categories = (\n",
        "        \"Classify the message according to the following categories:\\n\"\n",
        "        \"1. Medication Issues\\n\"\n",
        "        \"2. Symptom Updates & Clinical Concerns\\n\"\n",
        "        \"3. Medical Equipment, Supplies, and Home Health\\n\"\n",
        "        \"4. Administrative Tasks\\n\"\n",
        "        \"5. Lab Test & Imaging\\n\"\n",
        "        \"6. Appointment Scheduling / Rescheduling / Cancelling\\n\"\n",
        "        \"7. Caregiver Support and Logistics\\n\"\n",
        "        \"8. Specialist Referral related issues\\n\"\n",
        "        \"9. General Communications (confirmation, gratitude)\\n\"\n",
        "        \"10. General Communications: non-medical/logistics\\n\"\n",
        "        \"11. General Communications (other)\\n\"\n",
        "        \"Otherwise 0.\\n\\n\"\n",
        "        \"Output format:\\n\"\n",
        "        \"Classification: <0 to 11>\\n\"\n",
        "        \"Reason: <reason_text>\"\n",
        "    )\n",
        "\n",
        "    full_prompt = f\"{system_instruction}\\n\\n{categories}\\nMessage:\\n{message}\"\n",
        "\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=full_prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=temperature,\n",
        "                max_output_tokens=max_tokens,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        content = extract_text_from_response(response)\n",
        "\n",
        "        classification = 0\n",
        "        reasoning = \"No reason provided.\"\n",
        "\n",
        "        for line in content.splitlines():\n",
        "            line = line.strip()\n",
        "            if line.lower().startswith(\"classification\"):\n",
        "                try:\n",
        "                    classification = int(re.findall(r\"\\d+\", line.split(\":\", 1)[1])[0])\n",
        "                except Exception:\n",
        "                    classification = 0\n",
        "            elif line.lower().startswith(\"reason\"):\n",
        "                reasoning = line.split(\":\", 1)[1].strip()\n",
        "\n",
        "        if classification < 0 or classification > 11:\n",
        "            classification = 0\n",
        "\n",
        "        return classification, _sanitize_reason(reasoning)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Safe: do not print message or model output\n",
        "        _log(f\"[WARN] gemini_single_label failed: {type(e).__name__}\")\n",
        "        return 0, \"Error processing message.\"\n",
        "\n",
        "\n",
        "def gemini_multi_label_0_105(\n",
        "    client,\n",
        "    MODEL_ID: str,\n",
        "    message: str,\n",
        "    temperature: float = 0.0,\n",
        "    max_tokens: int = 2500,\n",
        ") -> Tuple[List[int], str]:\n",
        "    \"\"\"\n",
        "    Multi-label (up to 3) in 0–105.\n",
        "    SAFE_MODE: never prints model output; truncates returned rationale.\n",
        "    \"\"\"\n",
        "    _require_llm_enabled()\n",
        "    _raise_if_public_run_on_text()\n",
        "\n",
        "    system_instruction = (\n",
        "        \"These are the messages from patients sent to healthcare professionals.\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"1) If a message contains both medical and non-medical issues, prioritize medical topics.\\n\"\n",
        "        \"2) If, and only if, a message contains strictly non-medical topics, categorize it as 105.\\n\"\n",
        "        \"3) Prefer the most relevant category available. Up to 3 labels if truly necessary.\\n\"\n",
        "        \"Output format:\\n\"\n",
        "        \"Classification: <0 to 105>\\n\"\n",
        "        \"Reason: <reason_text>\"\n",
        "    )\n",
        "\n",
        "\n",
        "    categories = \"CATEGORIES: (see separate file / template)\\n\"\n",
        "\n",
        "    full_prompt = f\"{system_instruction}\\n\\n{categories}\\nMessage:\\n{message}\"\n",
        "\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=full_prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=temperature,\n",
        "                max_output_tokens=max_tokens,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        content = extract_text_from_response(response)\n",
        "\n",
        "        classifications: List[int] = [0]\n",
        "        reasoning = \"No reason provided.\"\n",
        "\n",
        "        for line in content.splitlines():\n",
        "            line = line.strip()\n",
        "            if line.lower().startswith(\"classification\"):\n",
        "                nums = [int(n) for n in re.findall(r\"\\d+\", line)]\n",
        "                clean: List[int] = []\n",
        "                for n in nums:\n",
        "                    if 0 <= n <= 105 and n not in clean:\n",
        "                        clean.append(n)\n",
        "                    if len(clean) == 3:\n",
        "                        break\n",
        "                classifications = clean or [0]\n",
        "            elif line.lower().startswith(\"reason\"):\n",
        "                reasoning = line.split(\":\", 1)[1].strip()\n",
        "\n",
        "        classifications = [c for c in classifications if 0 <= c <= 105] or [0]\n",
        "        return classifications, _sanitize_reason(reasoning)\n",
        "\n",
        "    except Exception as e:\n",
        "        _log(f\"[WARN] gemini_multi_label_0_105 failed: {type(e).__name__}\")\n",
        "        return [0], \"Error processing message.\"\n",
        "\n",
        "\n",
        "# =====================\n",
        "# IV. Dual ML pipeline (no path leakage; no text prints)\n",
        "# =====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load tables\n",
        "    messages = load_messages(MESSAGES_CSV)\n",
        "    symptoms = load_symptoms(SYMPTOMS_CSV)\n",
        "\n",
        "    # Ensure unique node IDs\n",
        "    messages = messages.drop_duplicates(subset=\"m_id\", keep=\"last\").copy()\n",
        "    symptoms = symptoms.drop_duplicates(subset=\"symptom_id\", keep=\"last\").copy()\n",
        "\n",
        "    keep_msg_cols = [c for c in [\"p_id\", \"m_id\", \"text\", \"timestamp\"] if c in messages.columns]\n",
        "    messages = messages[keep_msg_cols].copy()\n",
        "\n",
        "    p_ids_from_msgs = messages[\"p_id\"].astype(str).unique().tolist()\n",
        "\n",
        "    # Persons: demographics + comorbids\n",
        "    person_demo, demo_cols = load_person_demo(PERSON_DEMO_CSV, p_ids=p_ids_from_msgs, one_hot=True)\n",
        "    comorbids, icd_cols = load_comorbids(\n",
        "        COMORBID_CSV, p_ids=p_ids_from_msgs,\n",
        "        min_patients_per_code=ICD_MIN_PREV, max_codes=ICD_MAX_COLS\n",
        "    )\n",
        "\n",
        "    persons = person_demo.merge(comorbids, on=\"p_id\", how=\"left\").fillna(0)\n",
        "    person_feature_cols = demo_cols + icd_cols\n",
        "    persons = persons.drop_duplicates(subset=\"p_id\", keep=\"last\").copy()\n",
        "\n",
        "    # Temporal person features\n",
        "    persons, person_feature_cols = add_temporal_person_features(messages, persons, person_feature_cols)\n",
        "\n",
        "    # Build message→symptom edges\n",
        "    msg_emb = sym_emb = None\n",
        "    if USE_LLM_SCORES and LLM_SCORES_PATH.exists():\n",
        "        print(f\"[INFO] Using LLM scores at: {LLM_SCORES_PATH}\")\n",
        "        ms_edges = message_to_symptom_edges_from_llm(\n",
        "            messages, symptoms,\n",
        "            llm_scores_path=LLM_SCORES_PATH,\n",
        "            score_col=LLM_SCORE_COLUMN,\n",
        "            is_proba=LLM_IS_PROBA,\n",
        "            min_score=EDGE_MIN_SIM,\n",
        "            top_k=TOP_K_SYMPTOMS,\n",
        "        )\n",
        "\n",
        "        # Optional: precompute embeddings (only in memory)\n",
        "        msg_emb = embed_texts(messages[\"text\"].fillna(\"\").astype(str).tolist())\n",
        "        sym_emb = embed_texts(\n",
        "            (symptoms[\"symptom_name\"].astype(str) + \": \" + symptoms[\"description\"].astype(str)).tolist()\n",
        "        )\n",
        "\n",
        "        if BLEND_WITH_EMBEDDINGS:\n",
        "            emb_edges, _, _ = message_to_symptom_edges_via_embeddings(\n",
        "                messages, symptoms, top_k=TOP_K_SYMPTOMS, min_sim=EDGE_MIN_SIM\n",
        "            )\n",
        "            mix = ms_edges.merge(emb_edges, on=[\"m_id\", \"symptom_id\"], how=\"outer\", suffixes=(\"_llm\", \"_emb\")).fillna(0.0)\n",
        "            mix[\"weight\"] = BLEND_ALPHA * mix[\"weight_llm\"] + (1.0 - BLEND_ALPHA) * mix[\"weight_emb\"]\n",
        "            ms_edges = mix[[\"m_id\", \"symptom_id\", \"weight\"]]\n",
        "    else:\n",
        "        print(\"[INFO] LLM scores not found → using embeddings only.\")\n",
        "        ms_edges, msg_emb, sym_emb = message_to_symptom_edges_via_embeddings(\n",
        "            messages, symptoms, top_k=TOP_K_SYMPTOMS, min_sim=EDGE_MIN_SIM\n",
        "        )\n",
        "\n",
        "    # Keep edges that point to surviving nodes (AFTER ms_edges exists)\n",
        "    ms_edges = ms_edges[\n",
        "        ms_edges[\"m_id\"].astype(str).isin(messages[\"m_id\"].astype(str)) &\n",
        "        ms_edges[\"symptom_id\"].astype(str).isin(symptoms[\"symptom_id\"].astype(str))\n",
        "    ].copy()\n",
        "\n",
        "    # Negation down-weighting\n",
        "    ms_edges = apply_negation_downweight(messages, symptoms, ms_edges)\n",
        "\n",
        "    # Basic aggregate diagnostics (safe)\n",
        "    print(f\"[INFO] messages: n={len(messages):,}\")\n",
        "    print(f\"[INFO] symptoms: n={len(symptoms):,}\")\n",
        "    print(f\"[INFO] persons:  n={persons['p_id'].nunique():,}\")\n",
        "    print(f\"[INFO] ms_edges: n={len(ms_edges):,} (avg edges/msg ≈ {len(ms_edges)/max(len(messages),1):.2f})\")\n",
        "\n",
        "    # If PyG unavailable, do EN baseline in-memory + print only\n",
        "    if not _PYG_OK:\n",
        "        print(\"[WARN] torch_geometric not installed → skipping GNN.\")\n",
        "        X_sym = (ms_edges.merge(messages[[\"m_id\", \"p_id\"]], on=\"m_id\", how=\"left\")\n",
        "                        .pivot_table(index=\"p_id\", columns=\"symptom_id\", values=\"weight\", aggfunc=\"sum\", fill_value=0.0))\n",
        "        print(f\"[INFO] Built person×symptom matrix in-memory: shape={X_sym.shape} (NOT EXPORTED)\")\n",
        "\n",
        "        if RUN_EN_BASELINE_QUICK:\n",
        "            coefs, cv_auc = elastic_net_baseline(persons, X_sym, l1_ratio=EN_L1_RATIO, C=EN_C, cv=EN_CV)\n",
        "            print(f\"Elastic Net CV AUC (quick): {cv_auc:.3f}\")\n",
        "\n",
        "            top = coefs.abs().sort_values(ascending=False).head(20).rename(\"abs_coef\").reset_index()\n",
        "            top = top.rename(columns={\"index\": \"symptom_id\"}).merge(\n",
        "                symptoms[[\"symptom_id\", \"symptom_name\"]], on=\"symptom_id\", how=\"left\"\n",
        "            )\n",
        "            print(\"\\nTop 20 EN |coef| symptoms (aggregate):\")\n",
        "            print(top[[\"symptom_id\", \"symptom_name\", \"abs_coef\"]].to_string(index=False))\n",
        "        else:\n",
        "            print(\"[INFO] Quick EN baseline skipped.\")\n",
        "        raise SystemExit(0)\n",
        "\n",
        "    # Build base graph\n",
        "    hetero, id_maps = build_hetero_graph(\n",
        "        messages, ms_edges, symptoms,\n",
        "        persons_df=persons, person_feature_cols=person_feature_cols,\n",
        "        msg_emb=msg_emb, sym_emb=sym_emb,\n",
        "        add_message_knn=True, k_msg_sim=5, add_symptom_cooc=True,\n",
        "    )\n",
        "\n",
        "    # Build dx pairs from comorbid indicator matrix\n",
        "    if icd_cols:\n",
        "        dx_pairs = (persons[[\"p_id\"] + icd_cols]\n",
        "                    .melt(id_vars=[\"p_id\"], var_name=\"dx_id\", value_name=\"has_code\"))\n",
        "        dx_pairs = dx_pairs.loc[dx_pairs[\"has_code\"] > 0, [\"p_id\", \"dx_id\"]].copy()\n",
        "        hetero, id_maps = add_dx_to_graph(hetero, dx_pairs, id_maps)\n",
        "\n",
        "    # Person similarity edges\n",
        "    hetero = add_person_similarity_edges_safe(hetero, persons, person_feature_cols, id_maps, k=5, min_sim=0.25)\n",
        "\n",
        "    assert_graph_ok(hetero, id_maps)\n",
        "\n",
        "    # Train GNN\n",
        "    model, z, person_probs = train_gnn(hetero, persons, id_maps)\n",
        "\n",
        "    # PRINT-ONLY person_probs aggregate summary (no IDs)\n",
        "    person_probs = np.asarray(person_probs, dtype=float)\n",
        "    print(f\"[INFO] GNN person_probs (NOT EXPORTED): n={person_probs.size:,} \"\n",
        "          f\"mean={person_probs.mean():.4f} sd={person_probs.std():.4f} \"\n",
        "          f\"p50={np.quantile(person_probs, 0.50):.4f} p95={np.quantile(person_probs, 0.95):.4f}\")\n",
        "\n",
        "    # Symptom importance + EN baseline (computed in-memory)\n",
        "    sym_rank = rank_symptoms(messages, ms_edges, hetero, z, id_maps).merge(symptoms, on=\"symptom_id\", how=\"left\")\n",
        "    X_sym = build_person_symptom_matrix(messages, ms_edges, id_maps)\n",
        "\n",
        "    if RUN_EN_BASELINE_QUICK:\n",
        "        coefs, cv_auc = elastic_net_baseline(persons, X_sym, l1_ratio=EN_L1_RATIO, C=EN_C, cv=EN_CV)\n",
        "        print(f\"Elastic Net CV AUC (quick): {cv_auc:.3f}\")\n",
        "\n",
        "        coef_df = coefs.rename(\"elasticnet_coef\").reset_index().rename(columns={\"index\": \"symptom_id\"})\n",
        "        merged = sym_rank.merge(coef_df, on=\"symptom_id\", how=\"left\")\n",
        "        merged[\"elasticnet_coef_abs\"] = merged[\"elasticnet_coef\"].abs()\n",
        "\n",
        "        pipe_en, X_en, y_en = elastic_net_fit(persons, X_sym, l1_ratio=EN_L1_RATIO, C=EN_C)\n",
        "        perm_imp = elastic_net_permutation_importance(\n",
        "            pipe_en, X_en, y_en, feature_names=X_sym.columns.tolist(), n_repeats=10\n",
        "        )\n",
        "        perm_df = perm_imp.rename(\"en_perm_importance\").reset_index().rename(columns={\"index\": \"symptom_id\"})\n",
        "        merged = merged.merge(perm_df, on=\"symptom_id\", how=\"left\")\n",
        "\n",
        "        def _z(col):\n",
        "            arr = merged[col].fillna(0.0).values\n",
        "            mu, sd = arr.mean(), arr.std() + 1e-9\n",
        "            return (arr - mu) / sd\n",
        "\n",
        "        merged[\"event_assoc_score\"] = _z(\"en_perm_importance\")\n",
        "    else:\n",
        "        merged = sym_rank.copy()\n",
        "        for col in [\"elasticnet_coef\", \"elasticnet_coef_abs\", \"en_perm_importance\", \"event_assoc_score\"]:\n",
        "            merged[col] = np.nan\n",
        "        print(\"[INFO] Quick EN baseline skipped.\")\n",
        "\n",
        "    # Print top symptoms (aggregate only; no text, no IDs beyond symptom_id)\n",
        "    cols_show = [\n",
        "        \"symptom_id\", \"symptom_name\",\n",
        "        \"coverage\", \"coverage_recency\",\n",
        "        \"importance_score\",\n",
        "        \"en_perm_importance\",\n",
        "        \"event_assoc_score\",\n",
        "        \"elasticnet_coef\"\n",
        "    ]\n",
        "    cols_show = [c for c in cols_show if c in merged.columns]\n",
        "\n",
        "    top20 = (merged.sort_values(\"importance_score\", ascending=False)\n",
        "                   .loc[:, cols_show]\n",
        "                   .head(20)\n",
        "                   .fillna(0.0))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "MLawodM2fwu2"
      },
      "id": "MLawodM2fwu2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jt4LhBF4fwru"
      },
      "id": "jt4LhBF4fwru",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}