{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Supp\n",
        "\n"
      ],
      "metadata": {
        "id": "GYlto7qidlq0"
      },
      "id": "GYlto7qidlq0"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Redaction-safe logging\n",
        "# =====================\n",
        "def _log(msg: str) -> None:\n",
        "    # Only aggregate, non-sensitive logs in SAFE_MODE\n",
        "    if SAFE_MODE:\n",
        "        print(msg, flush=True)\n",
        "    else:\n",
        "        print(msg, flush=True)\n",
        "\n",
        "def _log_debug(msg: str) -> None:\n",
        "    if (not SAFE_MODE) and VERBOSE:\n",
        "        print(msg, flush=True)\n",
        "\n",
        "def _raise_if_public_run_on_text() -> None:\n",
        "    if SAFE_MODE and not ALLOW_TEXT_INPUT:\n",
        "        raise RuntimeError(\n",
        "            \"SAFE_MODE is ON and ALLOW_TEXT_INPUT is OFF.\\n\"\n",
        "            \"This public-safe script refuses to process real text by default.\\n\"\n",
        "            \"Flip ALLOW_TEXT_INPUT=True only in an approved secure environment.\"\n",
        "        )\n",
        "\n",
        "def _assert_columns(df: pd.DataFrame, required: List[str], name: str = \"df\") -> None:\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"{name} missing required columns: {missing}\")\n",
        "\n",
        "\n",
        "# =====================\n",
        "# I. Topic Mapping (prompt template only)\n",
        "# =====================\n",
        "UPDATE_SYSTEM = \"\"\"\n",
        "You are updating a 3-level clinical taxonomy for de-identified clinical\n",
        "portal messages: MAIN -> SUB1 -> SUB2.\n",
        "\n",
        "You must:\n",
        "- **KEEP** existing categories as much as possible.\n",
        "- ADD new MAIN/SUB1/SUB2 categories to **CAPTURE NEW TOPICS**\n",
        "- **DO NOT merge/remove** unless those are almost identical.\n",
        "- Keep the taxonomy interpretable and moderately granular.\n",
        "- For symptom taxonomy, be **especially GRANULAR**.\n",
        "- In the REASONING line, only describe what you added, merged, or renamed.\n",
        "Do NOT include general statements like “kept existing structure intact.”\n",
        "\n",
        "Write your response in plain text.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# =====================\n",
        "# II. BERTopic-based validation\n",
        "# =====================\n",
        "\n",
        "STOP = {\"and\",\"or\",\"the\",\"of\",\"for\",\"to\",\"in\",\"on\",\"a\",\"an\",\"with\",\"without\"}\n",
        "\n",
        "def normalize_text(x: str) -> str:\n",
        "    x = \"\" if pd.isna(x) else str(x)\n",
        "    return re.sub(r\"\\s+\", \" \", x.strip())\n",
        "\n",
        "def phrase_to_keywords(phrase: str) -> List[str]:\n",
        "    phrase = normalize_text(phrase).lower()\n",
        "    toks = re.findall(r\"[a-z]+\", phrase)\n",
        "    toks = [t for t in toks if t not in STOP and len(t) > 2]\n",
        "    return toks\n",
        "\n",
        "def build_seed_topic_list(seed_df: pd.DataFrame, main_col: str = \"main\", sub1_col: str = \"sub1\"):\n",
        "    \"\"\"\n",
        "    seed_df should be non-sensitive / publishable (or synthetic).\n",
        "    \"\"\"\n",
        "    if seed_df is None or len(seed_df) == 0:\n",
        "        raise ValueError(\"seed_df is empty. Provide a non-sensitive seed taxonomy (main/sub1).\")\n",
        "\n",
        "    _assert_columns(seed_df, [main_col, sub1_col], name=\"seed_df\")\n",
        "\n",
        "    tmp = seed_df[[main_col, sub1_col]].copy()\n",
        "    tmp[main_col] = tmp[main_col].map(normalize_text)\n",
        "    tmp[sub1_col] = tmp[sub1_col].map(normalize_text)\n",
        "    tmp = tmp.drop_duplicates(subset=[main_col, sub1_col])\n",
        "\n",
        "    grouped = tmp.groupby(main_col)[sub1_col].apply(list)\n",
        "\n",
        "    ontology_labels: List[str] = []\n",
        "    seed_topic_list: List[List[str]] = []\n",
        "\n",
        "    for main_label, sub1_list in grouped.items():\n",
        "        main_kw = phrase_to_keywords(main_label)\n",
        "        sub_kw: List[str] = []\n",
        "        for s in sub1_list:\n",
        "            sub_kw.extend(phrase_to_keywords(s))\n",
        "\n",
        "        seen, kws = set(), []\n",
        "        for w in (main_kw + sub_kw):\n",
        "            if w not in seen:\n",
        "                kws.append(w)\n",
        "                seen.add(w)\n",
        "\n",
        "        ontology_labels.append(main_label)\n",
        "        seed_topic_list.append(kws)\n",
        "\n",
        "    return ontology_labels, seed_topic_list, tmp\n",
        "\n",
        "\n",
        "# ---- embeddings (in-memory only) ----\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def compute_embeddings(texts: List[str], embedder_name: str = \"all-MiniLM-L6-v2\", batch_size: int = 128):\n",
        "    _raise_if_public_run_on_text()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    embedder = SentenceTransformer(embedder_name, device=device)\n",
        "    emb = embedder.encode(\n",
        "        list(texts),\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=(not SAFE_MODE),  # reduce chatter in public runs\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=False,\n",
        "    )\n",
        "    return embedder, emb\n",
        "\n",
        "\n",
        "# ---- BERTopic ----\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "def fit_guided_bertopic(df: pd.DataFrame, seed_topic_list, embedder, embeddings):\n",
        "    _raise_if_public_run_on_text()\n",
        "    _assert_columns(df, [ID_COL, TEXT_COL], name=\"df\")\n",
        "\n",
        "    vectorizer_model = CountVectorizer(\n",
        "        stop_words=\"english\",\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=2,\n",
        "        max_df=0.8,\n",
        "    )\n",
        "\n",
        "    umap_model = UMAP(\n",
        "        n_neighbors=30,\n",
        "        n_components=10,\n",
        "        min_dist=0.0,\n",
        "        metric=\"cosine\",\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    hdbscan_model = HDBSCAN(\n",
        "        min_cluster_size=30,\n",
        "        min_samples=10,\n",
        "        prediction_data=True,\n",
        "    )\n",
        "\n",
        "    model = BERTopic(\n",
        "        embedding_model=embedder,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        representation_model=KeyBERTInspired(),\n",
        "        seed_topic_list=seed_topic_list,\n",
        "        top_n_words=10,\n",
        "        calculate_probabilities=True,\n",
        "        verbose=False,\n",
        "        low_memory=True,\n",
        "    )\n",
        "\n",
        "    topics, probs = model.fit_transform(df[TEXT_COL].astype(str).tolist(), embeddings=embeddings)\n",
        "    info = model.get_topic_info()\n",
        "\n",
        "    # SAFE_MODE: never print topic names/words\n",
        "    _log(f\"[INFO] BERTopic fit complete: n_docs={len(df):,}, n_topics={(info['Topic']!=-1).sum():,}, outliers={(info['Topic']==-1).sum():,}\")\n",
        "    return model, topics, probs, info\n",
        "\n",
        "\n",
        "def _cosine_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)\n",
        "    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)\n",
        "    return A @ B.T\n",
        "\n",
        "def align_topics_to_ontology_safer(model, embedder, ontology_labels: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    SAFER alignment:\n",
        "    - Prefer topic centroid embeddings if available\n",
        "    - Otherwise use topic \"Name\" embeddings as fallback (still derived from corpus)\n",
        "    In SAFE_MODE, we will NOT export or print the Name strings.\n",
        "    \"\"\"\n",
        "    info = model.get_topic_info().copy()\n",
        "    info = info[info[\"Topic\"] != -1].reset_index(drop=True)\n",
        "    topic_ids = info[\"Topic\"].tolist()\n",
        "\n",
        "    topic_vecs = None\n",
        "\n",
        "    # Try BERTopic topic centroids\n",
        "    if hasattr(model, \"topic_embeddings_\") and model.topic_embeddings_ is not None:\n",
        "        topics_dict = model.get_topics()\n",
        "        ordered_topic_ids = [t for t in topics_dict.keys() if t != -1]\n",
        "        if len(ordered_topic_ids) == len(model.topic_embeddings_):\n",
        "            id_to_vec = {tid: model.topic_embeddings_[i] for i, tid in enumerate(ordered_topic_ids)}\n",
        "            vecs = []\n",
        "            for tid in topic_ids:\n",
        "                if tid in id_to_vec:\n",
        "                    vecs.append(id_to_vec[tid])\n",
        "            if len(vecs) == len(topic_ids):\n",
        "                topic_vecs = np.vstack(vecs)\n",
        "\n",
        "    # Fallback: encode topic Name (avoid returning the raw strings)\n",
        "    if topic_vecs is None:\n",
        "        names = info[\"Name\"].astype(str).tolist()\n",
        "        topic_vecs = np.asarray(embedder.encode(names, normalize_embeddings=True))\n",
        "\n",
        "    onto_vecs = np.asarray(embedder.encode(list(ontology_labels), normalize_embeddings=True))\n",
        "    sims = _cosine_sim_matrix(topic_vecs, onto_vecs)\n",
        "\n",
        "    best_idx = sims.argmax(axis=1)\n",
        "    best_sim = sims.max(axis=1)\n",
        "\n",
        "    aligned = pd.DataFrame({\n",
        "        \"topic_id\": topic_ids,\n",
        "        \"topic_size\": info[\"Count\"].tolist(),\n",
        "        \"main_label\": [ontology_labels[i] for i in best_idx],\n",
        "        \"alignment_score\": best_sim,\n",
        "    }).sort_values([\"main_label\", \"alignment_score\", \"topic_size\"], ascending=[True, False, False])\n",
        "\n",
        "    return aligned\n",
        "\n",
        "\n",
        "def run_bertopic_validation(df: pd.DataFrame, seed_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Returns aggregate artifacts only.\n",
        "    SAFE_MODE: does NOT return topic Name strings.\n",
        "    \"\"\"\n",
        "    _assert_columns(df, [ID_COL, TEXT_COL], name=\"df\")\n",
        "    _raise_if_public_run_on_text()\n",
        "\n",
        "    ontology_labels, seed_topic_list, seed_dedup = build_seed_topic_list(seed_df)\n",
        "    embedder, embeddings = compute_embeddings(df[TEXT_COL].astype(str).tolist())\n",
        "\n",
        "    model, topics, probs, info = fit_guided_bertopic(df, seed_topic_list, embedder, embeddings)\n",
        "    aligned_topics = align_topics_to_ontology_safer(model, embedder, ontology_labels)\n",
        "\n",
        "    # Drop Name in SAFE_MODE (topic names can leak)\n",
        "    if SAFE_MODE:\n",
        "        topic_info = info[[\"Topic\", \"Count\"]].copy()\n",
        "    else:\n",
        "        topic_info = info[[\"Topic\", \"Count\", \"Name\"]].copy()\n",
        "\n",
        "    return {\n",
        "        \"topic_info\": topic_info,\n",
        "        \"aligned_topics\": aligned_topics,\n",
        "        \"n_docs\": int(len(df)),\n",
        "        \"n_topics\": int((info[\"Topic\"] != -1).sum()),\n",
        "    }\n",
        "\n",
        "\n",
        "# =====================\n",
        "# III. Symptom annotation (Gemini) — safe wrappers\n",
        "# =====================\n",
        "\n",
        "try:\n",
        "    from google import genai\n",
        "    from google.genai import types\n",
        "except Exception:\n",
        "    genai = None\n",
        "    types = None\n",
        "\n",
        "\n",
        "def gemini_single_label(\n",
        "    client,\n",
        "    MODEL_ID: str,\n",
        "    message: str,\n",
        "    temperature: float = 0.0,\n",
        "    max_tokens: int = 2500,\n",
        ") -> Tuple[int, str]:\n",
        "    \"\"\"\n",
        "    Classify a single message into categories 0–11.\n",
        "    SAFE_MODE: never prints model output; truncates returned rationale.\n",
        "    \"\"\"\n",
        "    _require_llm_enabled()\n",
        "    _raise_if_public_run_on_text()\n",
        "\n",
        "    system_instruction = (\n",
        "        \"These are the messages from patients sent to healthcare professionals. \"\n",
        "        \"Rules: If a message contains both medical and non-medical issues, \"\n",
        "        \"prioritize addressing the medical topics (Symptom Updates & Clinical Concerns).\"\n",
        "    )\n",
        "\n",
        "    categories = (\n",
        "        \"Classify the message according to the following categories:\\n\"\n",
        "        \"1. Medication Issues\\n\"\n",
        "        \"2. Symptom Updates & Clinical Concerns\\n\"\n",
        "        \"3. Medical Equipment, Supplies, and Home Health\\n\"\n",
        "        \"4. Administrative Tasks\\n\"\n",
        "        \"5. Lab Test & Imaging\\n\"\n",
        "        \"6. Appointment Scheduling / Rescheduling / Cancelling\\n\"\n",
        "        \"7. Caregiver Support and Logistics\\n\"\n",
        "        \"8. Specialist Referral related issues\\n\"\n",
        "        \"9. General Communications (confirmation, gratitude)\\n\"\n",
        "        \"10. General Communications: non-medical/logistics\\n\"\n",
        "        \"11. General Communications (other)\\n\"\n",
        "        \"Otherwise 0.\\n\\n\"\n",
        "        \"Output format:\\n\"\n",
        "        \"Classification: <0 to 11>\\n\"\n",
        "        \"Reason: <reason_text>\"\n",
        "    )\n",
        "\n",
        "    full_prompt = f\"{system_instruction}\\n\\n{categories}\\nMessage:\\n{message}\"\n",
        "\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=full_prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=temperature,\n",
        "                max_output_tokens=max_tokens,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        content = extract_text_from_response(response)\n",
        "\n",
        "        classification = 0\n",
        "        reasoning = \"No reason provided.\"\n",
        "\n",
        "        for line in content.splitlines():\n",
        "            line = line.strip()\n",
        "            if line.lower().startswith(\"classification\"):\n",
        "                try:\n",
        "                    classification = int(re.findall(r\"\\d+\", line.split(\":\", 1)[1])[0])\n",
        "                except Exception:\n",
        "                    classification = 0\n",
        "            elif line.lower().startswith(\"reason\"):\n",
        "                reasoning = line.split(\":\", 1)[1].strip()\n",
        "\n",
        "        if classification < 0 or classification > 11:\n",
        "            classification = 0\n",
        "\n",
        "        return classification, _sanitize_reason(reasoning)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Safe: do not print message or model output\n",
        "        _log(f\"[WARN] gemini_single_label failed: {type(e).__name__}\")\n",
        "        return 0, \"Error processing message.\"\n",
        "\n",
        "\n",
        "def gemini_multi_label_0_105(\n",
        "    client,\n",
        "    MODEL_ID: str,\n",
        "    message: str,\n",
        "    temperature: float = 0.0,\n",
        "    max_tokens: int = 2500,\n",
        ") -> Tuple[List[int], str]:\n",
        "    \"\"\"\n",
        "    Multi-label (up to 3) in 0–105.\n",
        "    SAFE_MODE: never prints model output; truncates returned rationale.\n",
        "    \"\"\"\n",
        "    _require_llm_enabled()\n",
        "    _raise_if_public_run_on_text()\n",
        "\n",
        "    system_instruction = (\n",
        "        \"These are the messages from patients sent to healthcare professionals.\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"1) If a message contains both medical and non-medical issues, prioritize medical topics.\\n\"\n",
        "        \"2) If, and only if, a message contains strictly non-medical topics, categorize it as 105.\\n\"\n",
        "        \"3) Prefer the most relevant category available. Up to 3 labels if truly necessary.\\n\"\n",
        "        \"Output format:\\n\"\n",
        "        \"Classification: <0 to 105>\\n\"\n",
        "        \"Reason: <reason_text>\"\n",
        "    )\n",
        "\n",
        "\n",
        "    categories = \"CATEGORIES: (see separate file / template)\\n\"\n",
        "\n",
        "    full_prompt = f\"{system_instruction}\\n\\n{categories}\\nMessage:\\n{message}\"\n",
        "\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=full_prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=temperature,\n",
        "                max_output_tokens=max_tokens,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        content = extract_text_from_response(response)\n",
        "\n",
        "        classifications: List[int] = [0]\n",
        "        reasoning = \"No reason provided.\"\n",
        "\n",
        "        for line in content.splitlines():\n",
        "            line = line.strip()\n",
        "            if line.lower().startswith(\"classification\"):\n",
        "                nums = [int(n) for n in re.findall(r\"\\d+\", line)]\n",
        "                clean: List[int] = []\n",
        "                for n in nums:\n",
        "                    if 0 <= n <= 105 and n not in clean:\n",
        "                        clean.append(n)\n",
        "                    if len(clean) == 3:\n",
        "                        break\n",
        "                classifications = clean or [0]\n",
        "            elif line.lower().startswith(\"reason\"):\n",
        "                reasoning = line.split(\":\", 1)[1].strip()\n",
        "\n",
        "        classifications = [c for c in classifications if 0 <= c <= 105] or [0]\n",
        "        return classifications, _sanitize_reason(reasoning)\n",
        "\n",
        "    except Exception as e:\n",
        "        _log(f\"[WARN] gemini_multi_label_0_105 failed: {type(e).__name__}\")\n",
        "        return [0], \"Error processing message.\"\n",
        "\n",
        "\n",
        "# =====================\n",
        "# IV. Dual ML pipeline (no path leakage; no text prints)\n",
        "# =====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load tables\n",
        "    messages = load_messages(MESSAGES_CSV)\n",
        "    symptoms = load_symptoms(SYMPTOMS_CSV)\n",
        "\n",
        "    # Ensure unique node IDs\n",
        "    messages = messages.drop_duplicates(subset=\"m_id\", keep=\"last\").copy()\n",
        "    symptoms = symptoms.drop_duplicates(subset=\"symptom_id\", keep=\"last\").copy()\n",
        "\n",
        "    keep_msg_cols = [c for c in [\"p_id\", \"m_id\", \"text\", \"timestamp\"] if c in messages.columns]\n",
        "    messages = messages[keep_msg_cols].copy()\n",
        "\n",
        "    p_ids_from_msgs = messages[\"p_id\"].astype(str).unique().tolist()\n",
        "\n",
        "    # Persons: demographics + comorbids\n",
        "    person_demo, demo_cols = load_person_demo(PERSON_DEMO_CSV, p_ids=p_ids_from_msgs, one_hot=True)\n",
        "    comorbids, icd_cols = load_comorbids(\n",
        "        COMORBID_CSV, p_ids=p_ids_from_msgs,\n",
        "        min_patients_per_code=ICD_MIN_PREV, max_codes=ICD_MAX_COLS\n",
        "    )\n",
        "\n",
        "    persons = person_demo.merge(comorbids, on=\"p_id\", how=\"left\").fillna(0)\n",
        "    person_feature_cols = demo_cols + icd_cols\n",
        "    persons = persons.drop_duplicates(subset=\"p_id\", keep=\"last\").copy()\n",
        "\n",
        "    # Temporal person features\n",
        "    persons, person_feature_cols = add_temporal_person_features(messages, persons, person_feature_cols)\n",
        "\n",
        "    # Build message→symptom edges\n",
        "    msg_emb = sym_emb = None\n",
        "    if USE_LLM_SCORES and LLM_SCORES_PATH.exists():\n",
        "        print(f\"[INFO] Using LLM scores at: {LLM_SCORES_PATH}\")\n",
        "        ms_edges = message_to_symptom_edges_from_llm(\n",
        "            messages, symptoms,\n",
        "            llm_scores_path=LLM_SCORES_PATH,\n",
        "            score_col=LLM_SCORE_COLUMN,\n",
        "            is_proba=LLM_IS_PROBA,\n",
        "            min_score=EDGE_MIN_SIM,\n",
        "            top_k=TOP_K_SYMPTOMS,\n",
        "        )\n",
        "\n",
        "        # Optional: precompute embeddings (only in memory)\n",
        "        msg_emb = embed_texts(messages[\"text\"].fillna(\"\").astype(str).tolist())\n",
        "        sym_emb = embed_texts(\n",
        "            (symptoms[\"symptom_name\"].astype(str) + \": \" + symptoms[\"description\"].astype(str)).tolist()\n",
        "        )\n",
        "\n",
        "        if BLEND_WITH_EMBEDDINGS:\n",
        "            emb_edges, _, _ = message_to_symptom_edges_via_embeddings(\n",
        "                messages, symptoms, top_k=TOP_K_SYMPTOMS, min_sim=EDGE_MIN_SIM\n",
        "            )\n",
        "            mix = ms_edges.merge(emb_edges, on=[\"m_id\", \"symptom_id\"], how=\"outer\", suffixes=(\"_llm\", \"_emb\")).fillna(0.0)\n",
        "            mix[\"weight\"] = BLEND_ALPHA * mix[\"weight_llm\"] + (1.0 - BLEND_ALPHA) * mix[\"weight_emb\"]\n",
        "            ms_edges = mix[[\"m_id\", \"symptom_id\", \"weight\"]]\n",
        "    else:\n",
        "        print(\"[INFO] LLM scores not found → using embeddings only.\")\n",
        "        ms_edges, msg_emb, sym_emb = message_to_symptom_edges_via_embeddings(\n",
        "            messages, symptoms, top_k=TOP_K_SYMPTOMS, min_sim=EDGE_MIN_SIM\n",
        "        )\n",
        "\n",
        "    # Keep edges that point to surviving nodes (AFTER ms_edges exists)\n",
        "    ms_edges = ms_edges[\n",
        "        ms_edges[\"m_id\"].astype(str).isin(messages[\"m_id\"].astype(str)) &\n",
        "        ms_edges[\"symptom_id\"].astype(str).isin(symptoms[\"symptom_id\"].astype(str))\n",
        "    ].copy()\n",
        "\n",
        "    # Negation down-weighting\n",
        "    ms_edges = apply_negation_downweight(messages, symptoms, ms_edges)\n",
        "\n",
        "    # Basic aggregate diagnostics (safe)\n",
        "    print(f\"[INFO] messages: n={len(messages):,}\")\n",
        "    print(f\"[INFO] symptoms: n={len(symptoms):,}\")\n",
        "    print(f\"[INFO] persons:  n={persons['p_id'].nunique():,}\")\n",
        "    print(f\"[INFO] ms_edges: n={len(ms_edges):,} (avg edges/msg ≈ {len(ms_edges)/max(len(messages),1):.2f})\")\n",
        "\n",
        "    # If PyG unavailable, do EN baseline in-memory + print only\n",
        "    if not _PYG_OK:\n",
        "        print(\"[WARN] torch_geometric not installed → skipping GNN.\")\n",
        "        X_sym = (ms_edges.merge(messages[[\"m_id\", \"p_id\"]], on=\"m_id\", how=\"left\")\n",
        "                        .pivot_table(index=\"p_id\", columns=\"symptom_id\", values=\"weight\", aggfunc=\"sum\", fill_value=0.0))\n",
        "        print(f\"[INFO] Built person×symptom matrix in-memory: shape={X_sym.shape} (NOT EXPORTED)\")\n",
        "\n",
        "        if RUN_EN_BASELINE_QUICK:\n",
        "            coefs, cv_auc = elastic_net_baseline(persons, X_sym, l1_ratio=EN_L1_RATIO, C=EN_C, cv=EN_CV)\n",
        "            print(f\"Elastic Net CV AUC (quick): {cv_auc:.3f}\")\n",
        "\n",
        "            top = coefs.abs().sort_values(ascending=False).head(20).rename(\"abs_coef\").reset_index()\n",
        "            top = top.rename(columns={\"index\": \"symptom_id\"}).merge(\n",
        "                symptoms[[\"symptom_id\", \"symptom_name\"]], on=\"symptom_id\", how=\"left\"\n",
        "            )\n",
        "            print(\"\\nTop 20 EN |coef| symptoms (aggregate):\")\n",
        "            print(top[[\"symptom_id\", \"symptom_name\", \"abs_coef\"]].to_string(index=False))\n",
        "        else:\n",
        "            print(\"[INFO] Quick EN baseline skipped.\")\n",
        "        raise SystemExit(0)\n",
        "\n",
        "    # Build base graph\n",
        "    hetero, id_maps = build_hetero_graph(\n",
        "        messages, ms_edges, symptoms,\n",
        "        persons_df=persons, person_feature_cols=person_feature_cols,\n",
        "        msg_emb=msg_emb, sym_emb=sym_emb,\n",
        "        add_message_knn=True, k_msg_sim=5, add_symptom_cooc=True,\n",
        "    )\n",
        "\n",
        "    # Build dx pairs from comorbid indicator matrix\n",
        "    if icd_cols:\n",
        "        dx_pairs = (persons[[\"p_id\"] + icd_cols]\n",
        "                    .melt(id_vars=[\"p_id\"], var_name=\"dx_id\", value_name=\"has_code\"))\n",
        "        dx_pairs = dx_pairs.loc[dx_pairs[\"has_code\"] > 0, [\"p_id\", \"dx_id\"]].copy()\n",
        "        hetero, id_maps = add_dx_to_graph(hetero, dx_pairs, id_maps)\n",
        "\n",
        "    # Person similarity edges\n",
        "    hetero = add_person_similarity_edges_safe(hetero, persons, person_feature_cols, id_maps, k=5, min_sim=0.25)\n",
        "\n",
        "    assert_graph_ok(hetero, id_maps)\n",
        "\n",
        "    # Train GNN\n",
        "    model, z, person_probs = train_gnn(hetero, persons, id_maps)\n",
        "\n",
        "    # PRINT-ONLY person_probs aggregate summary (no IDs)\n",
        "    person_probs = np.asarray(person_probs, dtype=float)\n",
        "    print(f\"[INFO] GNN person_probs (NOT EXPORTED): n={person_probs.size:,} \"\n",
        "          f\"mean={person_probs.mean():.4f} sd={person_probs.std():.4f} \"\n",
        "          f\"p50={np.quantile(person_probs, 0.50):.4f} p95={np.quantile(person_probs, 0.95):.4f}\")\n",
        "\n",
        "    # Symptom importance + EN baseline (computed in-memory)\n",
        "    sym_rank = rank_symptoms(messages, ms_edges, hetero, z, id_maps).merge(symptoms, on=\"symptom_id\", how=\"left\")\n",
        "    X_sym = build_person_symptom_matrix(messages, ms_edges, id_maps)\n",
        "\n",
        "    if RUN_EN_BASELINE_QUICK:\n",
        "        coefs, cv_auc = elastic_net_baseline(persons, X_sym, l1_ratio=EN_L1_RATIO, C=EN_C, cv=EN_CV)\n",
        "        print(f\"Elastic Net CV AUC (quick): {cv_auc:.3f}\")\n",
        "\n",
        "        coef_df = coefs.rename(\"elasticnet_coef\").reset_index().rename(columns={\"index\": \"symptom_id\"})\n",
        "        merged = sym_rank.merge(coef_df, on=\"symptom_id\", how=\"left\")\n",
        "        merged[\"elasticnet_coef_abs\"] = merged[\"elasticnet_coef\"].abs()\n",
        "\n",
        "        pipe_en, X_en, y_en = elastic_net_fit(persons, X_sym, l1_ratio=EN_L1_RATIO, C=EN_C)\n",
        "        perm_imp = elastic_net_permutation_importance(\n",
        "            pipe_en, X_en, y_en, feature_names=X_sym.columns.tolist(), n_repeats=10\n",
        "        )\n",
        "        perm_df = perm_imp.rename(\"en_perm_importance\").reset_index().rename(columns={\"index\": \"symptom_id\"})\n",
        "        merged = merged.merge(perm_df, on=\"symptom_id\", how=\"left\")\n",
        "\n",
        "        def _z(col):\n",
        "            arr = merged[col].fillna(0.0).values\n",
        "            mu, sd = arr.mean(), arr.std() + 1e-9\n",
        "            return (arr - mu) / sd\n",
        "\n",
        "        merged[\"event_assoc_score\"] = _z(\"en_perm_importance\")\n",
        "    else:\n",
        "        merged = sym_rank.copy()\n",
        "        for col in [\"elasticnet_coef\", \"elasticnet_coef_abs\", \"en_perm_importance\", \"event_assoc_score\"]:\n",
        "            merged[col] = np.nan\n",
        "        print(\"[INFO] Quick EN baseline skipped.\")\n",
        "\n",
        "    # Print top symptoms (aggregate only; no text, no IDs beyond symptom_id)\n",
        "    cols_show = [\n",
        "        \"symptom_id\", \"symptom_name\",\n",
        "        \"coverage\", \"coverage_recency\",\n",
        "        \"importance_score\",\n",
        "        \"en_perm_importance\",\n",
        "        \"event_assoc_score\",\n",
        "        \"elasticnet_coef\"\n",
        "    ]\n",
        "    cols_show = [c for c in cols_show if c in merged.columns]\n",
        "\n",
        "    top20 = (merged.sort_values(\"importance_score\", ascending=False)\n",
        "                   .loc[:, cols_show]\n",
        "                   .head(20)\n",
        "                   .fillna(0.0))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "MLawodM2fwu2"
      },
      "id": "MLawodM2fwu2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jt4LhBF4fwru"
      },
      "id": "jt4LhBF4fwru",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}